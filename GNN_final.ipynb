{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# weight decay\n",
    "# undirected\n",
    "IRT_DIMS = 0\n",
    "DATASET = 'matrix'    \n",
    "ITEM_FEATURES = 'True'\n",
    "ITEM_FEATURES = False if ITEM_FEATURES == 'False' else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, shutil\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from utils import (mymode, load_data_heterogeneous, create_data_object_heterogeneous, create_data_object_heterogeneous_temporal)\n",
    "import seaborn as sns\n",
    "\n",
    "from IRT import MIRT_2PL\n",
    "from manage_experiments import perform_cross_validation\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'df_name': None, 'epochs': 10000, 'learning_rate': 0.005, 'weight_decay': 0.0, 'early_stopping': 200, 'n_splits': 10, 'device': 'cuda:0', 'batch_size': 8192, 'neighbours': [50, 50], 'model_type': 'GNN', 'hidden_dims': [8, 8]}\n",
      "matrix\n"
     ]
    }
   ],
   "source": [
    "# Initialise\n",
    "parameters = {\n",
    "    'df_name': None,\n",
    "    'epochs': 10000,\n",
    "    'learning_rate': 0.005,\n",
    "    'weight_decay': 0.0,\n",
    "    'early_stopping': 200,\n",
    "    'n_splits': 10,\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 2**13,\n",
    "#    'neighbours': [-1, -1]\n",
    "    'neighbours': [50, 50]\n",
    "    }\n",
    "\n",
    "if IRT_DIMS > 0:\n",
    "    parameters['model_type'] = 'IRT'\n",
    "    parameters['hidden_dims'] = IRT_DIMS\n",
    "    parameters['lambda1'] = 0\n",
    "    parameters['lambda2'] = 0\n",
    "    OUTNAME = 'IRT'\n",
    "else:\n",
    "    parameters['model_type'] = 'GNN'\n",
    "    parameters['hidden_dims'] = [8,8]\n",
    "    OUTNAME = 'SAGE' \n",
    "\n",
    "if ITEM_FEATURES:\n",
    "    OUTNAME = OUTNAME + '_scales'\n",
    "    \n",
    "print(parameters)\n",
    "print(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'data/mindsteps_set_' + DATASET\n",
    "df = load_data_heterogeneous(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IRT_DIMS > 0:\n",
    "    data, df_student, df_item, df_edge = create_data_object_heterogeneous(df, return_aux_data=True, item_features=ITEM_FEATURES)\n",
    "else:\n",
    "    data, df_student, df_student_age, df_item, df_edge = create_data_object_heterogeneous_temporal(df, return_aux_data=True, item_features=ITEM_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mstudent\u001b[0m={\n",
      "    node_id=[636327],\n",
      "    x=[636327, 4]\n",
      "  },\n",
      "  \u001b[1mitem\u001b[0m={\n",
      "    node_id=[17868],\n",
      "    x=[17868, 11]\n",
      "  },\n",
      "  \u001b[1m(student, responds, item)\u001b[0m={\n",
      "    edge_index=[2, 5613692],\n",
      "    edge_attr=[5613692, 2],\n",
      "    y=[5613692]\n",
      "  },\n",
      "  \u001b[1m(item, rev_responds, student)\u001b[0m={ edge_index=[2, 5613692] },\n",
      "  \u001b[1m(student, preceeds, student)\u001b[0m={ edge_index=[2, 584819] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from Heterogeneous_temporal_embedder import EmbedderHeterogeneous, train_embedder_heterogeneous, test_embedder_heterogeneous\n",
    "model = EmbedderHeterogeneous( \n",
    "    n_students = data['student'].node_id.size(0),\n",
    "    n_items = data['item'].node_id.size(0),\n",
    "    student_inchannel = data['student'].x.size(1),\n",
    "    item_inchannel = data['item'].x.size(1),\n",
    "    hidden_channels = parameters['hidden_dims'],\n",
    "    edge_channel = 2,\n",
    "    metadata = data.metadata()\n",
    "    ).to(device)\n",
    "#model(data)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=parameters['learning_rate'], weight_decay=parameters['weight_decay'])\n",
    "criterion = torch.nn.BCEWithLogitsLoss().to(device)#class_weights)\n",
    "loss = []\n",
    "for i in range(3000):\n",
    "    l = train_embedder_heterogeneous(model, data, optimizer, criterion)\n",
    "    loss.append(l)\n",
    "    if i % 100 == 0:\n",
    "        print(i, l)\n",
    "        metrics = test_embedder_heterogeneous(model, data, 0, 'test')\n",
    "        print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_losses = torch.Tensor(loss).detach().numpy()\n",
    "train_indices = np.arange(len(train_losses))\n",
    "fig = plt.figure()\n",
    "plt.plot(train_indices, train_losses, c='blue')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "WARNING: running with a fixed random state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters\n",
      "{'n_students': 636327, 'n_items': 17868, 'student_inchannel': 4, 'item_inchannel': 11, 'hidden_channels': [8, 8], 'edge_channel': 2}\n",
      "[8, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/78 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 16/78 [00:31<02:00,  1.94s/it]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 16/78 [00:41<02:00,  1.94s/it]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▎     | 34/78 [01:02<01:20,  1.82s/it]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▎     | 34/78 [01:21<01:20,  1.82s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 52/78 [01:33<00:46,  1.77s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 52/78 [01:51<00:46,  1.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 78/78 [02:13<00:00,  1.71s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 001, Loss: 0.4660, Val: 0.6007, Test: 0.6054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/10000 [02:14<374:15:01, 134.74s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/78 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 20/78 [00:31<01:30,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 20/78 [00:46<01:30,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 39/78 [01:01<01:02,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 39/78 [01:16<01:02,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 53/78 [01:33<00:46,  1.85s/it]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 53/78 [01:46<00:46,  1.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 78/78 [02:07<00:00,  1.63s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 002, Loss: 0.4257, Val: 0.6755, Test: 0.6785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 2/10000 [04:23<364:02:31, 131.08s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/78 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 20/78 [00:30<01:27,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 20/78 [00:48<01:27,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████▏    | 40/78 [01:00<00:57,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████▏    | 40/78 [01:18<00:57,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 60/78 [01:31<00:27,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 78/78 [01:56<00:00,  1.49s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 003, Loss: 0.4007, Val: 0.7018, Test: 0.7035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 3/10000 [06:20<346:40:58, 124.84s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/78 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 20/78 [00:31<01:30,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 20/78 [00:50<01:30,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████▏    | 40/78 [01:03<01:00,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████▏    | 40/78 [01:20<01:00,  1.58s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from manage_experiments import perform_cross_validation\n",
    "output_dict, model = perform_cross_validation(data, parameters, save_embeddings=True, save_subgraph=True, final_fit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict.keys()\n",
    "print('AUC:', output_dict['AUC_0_test'])\n",
    "print('Balanced Accuracy:', output_dict['Balanced Accuracy_0_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_scales = df_item['scale'].unique()\n",
    "unique_domains = df_item['domain'].unique()\n",
    "unique_matdiff = df_item['matdiff'].sort_values().unique()\n",
    "#scale_colors = dict([(c, plt.cm.tab10(i)) for i, c in enumerate(unique_scales)])\n",
    "#domain_colors = dict([(c, plt.cm.tab10(i)) for i, c in enumerate(unique_domains)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = output_dict['losses_0']\n",
    "train_edge_indices, val_edge_indices, test_edge_indices = output_dict['indices_0']\n",
    "\n",
    "train_data = output_dict['train_subgraph_data'] \n",
    "val_data = output_dict['val_subgraph_data'] \n",
    "test_data = output_dict['test_subgraph_data']\n",
    "\n",
    "aux_data = (df, df_student, df_item, df_edge, \n",
    "    #clustering_indices, \n",
    "    train_losses, #test_losses, test_aucs, \n",
    "    train_edge_indices, val_edge_indices, test_edge_indices, \n",
    "    data.cpu(), train_data.cpu(), val_data.cpu(), test_data.cpu(),\n",
    "    unique_scales, unique_domains, unique_matdiff, \n",
    "    DATA_FILE)\n",
    "\n",
    "with open(f'./results/{OUTNAME}_{DATASET}_aux_data.pkl', 'wb') as handle:\n",
    "    pickle.dump(aux_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE EVERYTHING\n",
    "torch.save(model, f'./results/{OUTNAME}_{DATASET}.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.arange(len(train_losses))\n",
    "fig = plt.figure()\n",
    "plt.plot(train_indices, train_losses, c='blue')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save IRT parameters to matrix\n",
    "if OUTNAME == 'IRT' and IRT_DIMS == 1:    \n",
    "    #z_dict = model.get_embeddings(train_data.to(device))\n",
    "    z_dict = output_dict['embedding_0']\n",
    "    df_item['IRT1_difficulty'] = -z_dict['offset']\n",
    "    df_item['IRT1_discrimination'] = z_dict['discrimination']\n",
    "    df_item['IRT1_discrimination_transf'] = z_dict['item']\n",
    "    ability = z_dict['ability']\n",
    "    df_edge['IRT1_ability'] = ability.ravel()\n",
    "    \n",
    "    aux_data = (df, df_student, df_item, df_edge, \n",
    "    train_losses, \n",
    "    train_edge_indices, val_edge_indices, test_edge_indices,\n",
    "    data.cpu(), train_data.cpu(), val_data.cpu().cpu(), test_data.cpu(),\n",
    "    unique_scales, unique_domains, unique_matdiff, \n",
    "    DATA_FILE)\n",
    "\n",
    "    with open(f'./results/{OUTNAME}_{DATASET}_aux_data_IRT1.pkl', 'wb') as handle:\n",
    "        pickle.dump(aux_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    df_item_clean = df_item.dropna(subset=['IRT_difficulty', 'IRT1_difficulty'])    \n",
    "    \n",
    "    fig = plt.figure()\n",
    "    sns.scatterplot(x='IRT_difficulty', y='IRT1_difficulty', data=df_item, hue='scale')\n",
    "    plt.title('Difficulty')\n",
    "    print('Difficulty:', pearsonr(df_item_clean['IRT1_difficulty'], df_item_clean['IRT_difficulty']))\n",
    "    \n",
    "    edge_feat = train_data['student', 'responds', 'item'].edge_attr.detach().cpu().numpy()\n",
    "    df_edge_clean = df_edge.dropna(subset=['IRT1_ability', 'ability', 'age'])\n",
    "    fig = plt.figure()\n",
    "    sns.scatterplot(x='age', y='IRT1_ability', data=df_edge_clean, hue='grade')\n",
    "    plt.title('Age-Ability')\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    sns.scatterplot(x='grade', y='IRT1_ability', data=df_edge_clean, hue='age')\n",
    "    plt.title('Grade-Ability')\n",
    "    print('Age-Ability:', pearsonr(df_edge_clean['age'], df_edge_clean['IRT1_ability']))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    sns.scatterplot(x='ability', y='IRT1_ability', data=df_edge_clean, hue='grade')\n",
    "    plt.title('Ability')\n",
    "    print('Ability:', pearsonr(df_edge_clean['ability'], df_edge_clean['IRT1_ability']))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "88516cc94b965045253aac22be7e673e07faa374a8dfeab45aefc65ddf94d8b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
