{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "IRT_DIMS = 0\n",
    "DATASET = 'matrix'    \n",
    "ITEM_FEATURES = 'True'\n",
    "ITEM_FEATURES = False if ITEM_FEATURES == 'False' else True\n",
    "\n",
    "# add layer for reverse\n",
    "# IRT or use common student embeddings\n",
    "# common embedding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, shutil\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from utils import (mymode, load_data_heterogeneous, create_data_object_heterogeneous, create_data_object_heterogeneous_temporal)\n",
    "import seaborn as sns\n",
    "\n",
    "from IRT import MIRT_2PL\n",
    "from manage_experiments import perform_cross_validation\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'df_name': None, 'epochs': 10000, 'learning_rate': 0.005, 'weight_decay': 0, 'early_stopping': 200, 'n_splits': 10, 'device': 'cuda:0', 'batch_size': 2048, 'neighbours': [30, 30], 'model_type': 'GNN', 'hidden_dims': [16, 8]}\n",
      "matrix\n"
     ]
    }
   ],
   "source": [
    "# Initialise\n",
    "parameters = {\n",
    "    'df_name': None,\n",
    "    'epochs': 10000,\n",
    "    'learning_rate': 0.005,\n",
    "    'weight_decay': 0,\n",
    "    'early_stopping': 200,\n",
    "    'n_splits': 10,\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 2**11,\n",
    "#    'neighbours': [-1, -1]\n",
    "    'neighbours': [30, 30]\n",
    "    }\n",
    "\n",
    "if IRT_DIMS > 0:\n",
    "    parameters['model_type'] = 'IRT'\n",
    "    parameters['hidden_dims'] = IRT_DIMS\n",
    "    parameters['lambda1'] = 0\n",
    "    parameters['lambda2'] = 0\n",
    "    OUTNAME = 'IRT'\n",
    "else:\n",
    "    parameters['model_type'] = 'GNN'\n",
    "    parameters['hidden_dims'] = [16,8]\n",
    "    OUTNAME = 'SAGE' \n",
    "\n",
    "if ITEM_FEATURES:\n",
    "    OUTNAME = OUTNAME + '_scales'\n",
    "    \n",
    "print(parameters)\n",
    "print(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'data/mindsteps_set_' + DATASET\n",
    "df = load_data_heterogeneous(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IRT_DIMS > 0:\n",
    "        data, df_student, df_item, df_edge = create_data_object_heterogeneous(df, return_aux_data=True, item_features=ITEM_FEATURES)\n",
    "    else:\n",
    "        data, df_student, df_student_age, df_item, df_edge = create_data_object_heterogeneous_temporal(df, return_aux_data=True, item_features=ITEM_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mstudent\u001b[0m={\n",
      "    node_id=[636327],\n",
      "    x=[636327, 3]\n",
      "  },\n",
      "  \u001b[1mitem\u001b[0m={\n",
      "    node_id=[17868],\n",
      "    x=[17868, 11]\n",
      "  },\n",
      "  \u001b[1m(student, responds, item)\u001b[0m={\n",
      "    edge_index=[2, 5613692],\n",
      "    edge_attr=[5613692, 2],\n",
      "    y=[5613692]\n",
      "  },\n",
      "  \u001b[1m(item, rev_responds, student)\u001b[0m={ edge_index=[2, 5613692] },\n",
      "  \u001b[1m(student, preceeds, student)\u001b[0m={\n",
      "    edge_index=[2, 584819],\n",
      "    edge_attr=[584819, 2]\n",
      "  }\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        ...,\n",
       "        [0.0000e+00, 0.0000e+00, 5.1506e+04],\n",
       "        [0.0000e+00, 1.0000e+00, 5.1507e+04],\n",
       "        [0.0000e+00, 1.0000e+00, 5.1507e+04]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from Heterogeneous_temporal_embedder import EmbedderHeterogeneous, train_embedder_heterogeneous, test_embedder_heterogeneous\n",
    "model = EmbedderHeterogeneous( \n",
    "    n_students = data['student'].node_id.size(0),\n",
    "    n_items = data['item'].node_id.size(0),\n",
    "    student_inchannel = data['student'].x.size(1),\n",
    "    item_inchannel = data['item'].x.size(1),\n",
    "    hidden_channels = parameters['hidden_dims'],\n",
    "    edge_channel = 2,\n",
    "    metadata = data.metadata()\n",
    "    ).to(device)\n",
    "#model(data)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=parameters['learning_rate'], weight_decay=parameters['weight_decay'])\n",
    "criterion = torch.nn.BCEWithLogitsLoss().to(device)#class_weights)\n",
    "loss = []\n",
    "for i in range(3000):\n",
    "    l = train_embedder_heterogeneous(model, data, optimizer, criterion)\n",
    "    loss.append(l)\n",
    "    if i % 100 == 0:\n",
    "        print(i, l)\n",
    "        metrics = test_embedder_heterogeneous(model, data, 0, 'test')\n",
    "        print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_losses = torch.Tensor(loss).detach().numpy()\n",
    "train_indices = np.arange(len(train_losses))\n",
    "fig = plt.figure()\n",
    "plt.plot(train_indices, train_losses, c='blue')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "WARNING: running with a fixed random state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters\n",
      "{'n_students': 636327, 'n_items': 17868, 'student_inchannel': 3, 'item_inchannel': 11, 'hidden_channels': [16, 8], 'edge_channel': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/311 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 7/311 [00:10<07:37,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 21/311 [00:20<04:32,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 21/311 [00:31<04:32,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 38/311 [00:31<03:26,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 56/311 [00:41<02:52,  1.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 74/311 [00:53<02:35,  1.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 91/311 [01:04<02:23,  1.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 107/311 [01:14<02:11,  1.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 126/311 [01:24<01:52,  1.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 145/311 [01:34<01:37,  1.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 164/311 [01:45<01:25,  1.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 183/311 [01:56<01:12,  1.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 202/311 [02:08<01:04,  1.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 218/311 [02:18<00:55,  1.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 237/311 [02:28<00:43,  1.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 256/311 [02:40<00:33,  1.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 278/311 [02:50<00:18,  1.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 311/311 [03:10<00:00,  1.63it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUC_0_val': 0.7860767067539696, 'Confusion_0_val': [[77823, 26291], [57635, 118936]], 'Balanced Accuracy_0_val': 0.7105330603410727, 'fold': 0} {'AUC_0_test': 0.7877243718828381, 'Confusion_0_test': [[78054, 26098], [57793, 118740]], 'Balanced Accuracy_0_test': 0.7110230174330475, 'fold': 0}\n",
      "\n",
      "Epoch: 001, Loss: 0.3940, Val: 0.7105, Test: 0.7110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/10000 [03:15<543:49:10, 195.79s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/311 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 10/311 [00:10<05:11,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 23/311 [00:20<04:08,  1.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 37/311 [00:30<03:41,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 53/311 [00:41<03:09,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 69/311 [00:51<02:48,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 86/311 [01:01<02:28,  1.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 103/311 [01:12<02:15,  1.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 121/311 [01:22<01:58,  1.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 139/311 [01:32<01:44,  1.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 157/311 [01:44<01:35,  1.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 157/311 [01:55<01:35,  1.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 173/311 [01:55<01:28,  1.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 193/311 [02:06<01:10,  1.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 213/311 [02:18<00:59,  1.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▍  | 233/311 [02:28<00:45,  1.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████▏ | 253/311 [02:39<00:32,  1.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 275/311 [02:49<00:18,  1.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 311/311 [03:06<00:00,  1.67it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUC_0_val': 0.7895078110362401, 'Confusion_0_val': [[78985, 25129], [58777, 117794]], 'Balanced Accuracy_0_val': 0.7128796551828431, 'fold': 0} {'AUC_0_test': 0.791037294814548, 'Confusion_0_test': [[79185, 24967], [58821, 117712]], 'Balanced Accuracy_0_test': 0.7135409449979833, 'fold': 0}\n",
      "\n",
      "Epoch: 002, Loss: 0.3921, Val: 0.7129, Test: 0.7135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 2/10000 [06:23<530:05:58, 190.87s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/311 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 17/311 [00:10<02:58,  1.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 34/311 [00:21<02:55,  1.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 50/311 [00:31<02:47,  1.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 68/311 [00:42<02:29,  1.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 86/311 [00:53<02:21,  1.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 102/311 [01:06<02:23,  1.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 119/311 [01:17<02:07,  1.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▎     | 136/311 [01:27<01:52,  1.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▎     | 136/311 [01:38<01:52,  1.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 152/311 [01:38<01:42,  1.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 170/311 [01:48<01:27,  1.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 189/311 [01:58<01:12,  1.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 189/311 [02:18<01:12,  1.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 206/311 [02:18<01:20,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 227/311 [02:28<00:56,  1.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████▉  | 248/311 [02:42<00:41,  1.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 270/311 [02:52<00:24,  1.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 311/311 [03:13<00:00,  1.61it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUC_0_val': 0.7906953657701135, 'Confusion_0_val': [[79709, 24405], [59764, 116807]], 'Balanced Accuracy_0_val': 0.7135617034363997, 'fold': 0} {'AUC_0_test': 0.7921856964057153, 'Confusion_0_test': [[80007, 24145], [59802, 116731]], 'Balanced Accuracy_0_test': 0.7147085832856572, 'fold': 0}\n",
      "\n",
      "Epoch: 003, Loss: 0.3908, Val: 0.7136, Test: 0.7147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 3/10000 [09:37<534:50:11, 192.60s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/311 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 16/311 [00:10<03:17,  1.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 31/311 [00:20<03:08,  1.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 47/311 [00:30<02:52,  1.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 63/311 [00:42<02:51,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 63/311 [00:53<02:51,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 77/311 [00:53<02:48,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 90/311 [01:07<03:04,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 100/311 [01:19<03:12,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 116/311 [01:29<02:39,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 132/311 [01:40<02:16,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 148/311 [01:52<02:03,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 164/311 [02:02<01:46,  1.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 184/311 [02:13<01:22,  1.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 184/311 [02:23<01:22,  1.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 203/311 [02:23<01:06,  1.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 223/311 [02:33<00:51,  1.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 243/311 [02:44<00:38,  1.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▍ | 264/311 [02:54<00:25,  1.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 285/311 [03:04<00:13,  1.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 311/311 [03:19<00:00,  1.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 4/10000 [12:57<542:52:29, 195.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUC_0_val': 0.7853764285245489, 'Confusion_0_val': [[78918, 25196], [59867, 116704]], 'Balanced Accuracy_0_val': 0.7094713154286505, 'fold': 0} {'AUC_0_test': 0.7864908896894582, 'Confusion_0_test': [[79111, 25041], [59976, 116557]], 'Balanced Accuracy_0_test': 0.7099143519437673, 'fold': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/311 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 15/311 [00:10<03:21,  1.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 30/311 [00:20<03:15,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 45/311 [00:31<03:05,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 45/311 [00:43<03:05,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▊        | 58/311 [00:46<03:36,  1.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 72/311 [00:56<03:15,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▊       | 89/311 [01:06<02:43,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 106/311 [01:18<02:26,  1.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 124/311 [01:28<02:03,  1.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 142/311 [01:39<01:49,  1.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 159/311 [01:50<01:37,  1.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 177/311 [02:00<01:22,  1.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 195/311 [02:11<01:10,  1.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 195/311 [02:23<01:10,  1.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 210/311 [02:24<01:08,  1.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▍  | 233/311 [02:34<00:46,  1.69it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 256/311 [02:45<00:30,  1.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 277/311 [02:55<00:18,  1.87it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from manage_experiments import perform_cross_validation\n",
    "output_dict, model = perform_cross_validation(data, parameters, save_embeddings=True, save_subgraph=True, final_fit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutput_dict\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUC:\u001b[39m\u001b[38;5;124m'\u001b[39m, output_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUC_0_test\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBalanced Accuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, output_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBalanced Accuracy_0_test\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_dict' is not defined"
     ]
    }
   ],
   "source": [
    "output_dict.keys()\n",
    "print('AUC:', output_dict['AUC_0_test'])\n",
    "print('Balanced Accuracy:', output_dict['Balanced Accuracy_0_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_scales = df_item['scale'].unique()\n",
    "unique_domains = df_item['domain'].unique()\n",
    "unique_matdiff = df_item['matdiff'].sort_values().unique()\n",
    "#scale_colors = dict([(c, plt.cm.tab10(i)) for i, c in enumerate(unique_scales)])\n",
    "#domain_colors = dict([(c, plt.cm.tab10(i)) for i, c in enumerate(unique_domains)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = output_dict['losses_0']\n",
    "train_edge_indices, val_edge_indices, test_edge_indices = output_dict['indices_0']\n",
    "\n",
    "train_data = output_dict['train_subgraph_data'] \n",
    "val_data = output_dict['val_subgraph_data'] \n",
    "test_data = output_dict['test_subgraph_data']\n",
    "\n",
    "aux_data = (df, df_student, df_item, df_edge, \n",
    "    #clustering_indices, \n",
    "    train_losses, #test_losses, test_aucs, \n",
    "    train_edge_indices, val_edge_indices, test_edge_indices, \n",
    "    data.cpu(), train_data.cpu(), val_data.cpu(), test_data.cpu(),\n",
    "    unique_scales, unique_domains, unique_matdiff, \n",
    "    DATA_FILE)\n",
    "\n",
    "with open(f'./results/{OUTNAME}_{DATASET}_aux_data.pkl', 'wb') as handle:\n",
    "    pickle.dump(aux_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE EVERYTHING\n",
    "torch.save(model, f'./results/{OUTNAME}_{DATASET}.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.arange(len(train_losses))\n",
    "fig = plt.figure()\n",
    "plt.plot(train_indices, train_losses, c='blue')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save IRT parameters to matrix\n",
    "if OUTNAME == 'IRT' and IRT_DIMS == 1:    \n",
    "    #z_dict = model.get_embeddings(train_data.to(device))\n",
    "    z_dict = output_dict['embedding_0']\n",
    "    df_item['IRT1_difficulty'] = -z_dict['offset']\n",
    "    df_item['IRT1_discrimination'] = z_dict['discrimination']\n",
    "    df_item['IRT1_discrimination_transf'] = z_dict['item']\n",
    "    ability = z_dict['ability']\n",
    "    df_edge['IRT1_ability'] = ability.ravel()\n",
    "    \n",
    "    aux_data = (df, df_student, df_item, df_edge, \n",
    "    train_losses, \n",
    "    train_edge_indices, val_edge_indices, test_edge_indices,\n",
    "    data.cpu(), train_data.cpu(), val_data.cpu().cpu(), test_data.cpu(),\n",
    "    unique_scales, unique_domains, unique_matdiff, \n",
    "    DATA_FILE)\n",
    "\n",
    "    with open(f'./results/{OUTNAME}_{DATASET}_aux_data_IRT1.pkl', 'wb') as handle:\n",
    "        pickle.dump(aux_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    df_item_clean = df_item.dropna(subset=['IRT_difficulty', 'IRT1_difficulty'])    \n",
    "    \n",
    "    fig = plt.figure()\n",
    "    sns.scatterplot(x='IRT_difficulty', y='IRT1_difficulty', data=df_item, hue='scale')\n",
    "    plt.title('Difficulty')\n",
    "    print('Difficulty:', pearsonr(df_item_clean['IRT1_difficulty'], df_item_clean['IRT_difficulty']))\n",
    "    \n",
    "    edge_feat = train_data['student', 'responds', 'item'].edge_attr.detach().cpu().numpy()\n",
    "    df_edge_clean = df_edge.dropna(subset=['IRT1_ability', 'ability', 'age'])\n",
    "    fig = plt.figure()\n",
    "    sns.scatterplot(x='age', y='IRT1_ability', data=df_edge_clean, hue='grade')\n",
    "    plt.title('Age-Ability')\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    sns.scatterplot(x='grade', y='IRT1_ability', data=df_edge_clean, hue='age')\n",
    "    plt.title('Grade-Ability')\n",
    "    print('Age-Ability:', pearsonr(df_edge_clean['age'], df_edge_clean['IRT1_ability']))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    sns.scatterplot(x='ability', y='IRT1_ability', data=df_edge_clean, hue='grade')\n",
    "    plt.title('Ability')\n",
    "    print('Ability:', pearsonr(df_edge_clean['ability'], df_edge_clean['IRT1_ability']))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "88516cc94b965045253aac22be7e673e07faa374a8dfeab45aefc65ddf94d8b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
