{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathpy as pp\n",
    "\n",
    "def generate_bipartite_block_probs(dict_p_stud_to_task):\n",
    "    n_groups_from_dict_values = set(len(v) for v in dict_p_stud_to_task.values())\n",
    "    assert len(n_groups_from_dict_values)==1, \"Input dict_p_stud_to_task implies varying number of groups\"\n",
    "    n_groups= list(n_groups_from_dict_values)[0]\n",
    "    assert n_groups==len(dict_p_stud_to_task), \"not enough keys\"\n",
    "\n",
    "    bip_block_probs = np.zeros((2*n_groups, 2*n_groups))\n",
    "    # Fill in the probabilities within and between groups\n",
    "    # Only filling probs of students to interact with tasks. \n",
    "    # Since the interactions are symmetric, the information on tasks interacting with studetns is redundant and can be obtained by summing with the transpose (see below )\n",
    "    #\n",
    "    # even indexes for students\n",
    "    for row in range(0,2*n_groups,2):\n",
    "        # odd indexes are for tasks\n",
    "        for col in range(1,2*n_groups,2):\n",
    "            stud_ix = row//2\n",
    "            task_ix = int(col/2)\n",
    "            bip_block_probs[row,col] = dict_p_stud_to_task[stud_ix][task_ix]\n",
    "    return bip_block_probs + bip_block_probs.T\n",
    "\n",
    "\n",
    "\n",
    "def sample_dcsbm(num_nodes, B, expected_degrees):\n",
    "    \"\"\"\n",
    "    Samples a network from a degree-corrected stochastic block model\n",
    "    \"\"\"\n",
    "    # assert False, \"check that num blcks an block matrix agree\"\n",
    "    assert B.shape[0] == B.shape[1], \"B should be square\"\n",
    "    num_blocks = B.shape[0]\n",
    "    m = np.random.choice(num_blocks, size=num_nodes)\n",
    "\n",
    "    # generate edges\n",
    "    edge_list = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i+1, num_nodes):\n",
    "            p_edge = B[m[i], m[j]]*expected_degrees[i]*expected_degrees[j]/(sum(expected_degrees))\n",
    "            p_edge = min(p_edge, 1)\n",
    "            assert p_edge <= 1, f\"Edge probability larger than 1: {p_edge}\"\n",
    "            if np.random.rand() < p_edge:\n",
    "                edge_list.append((i, j))\n",
    "\n",
    "    # Create a graph and add nodes\n",
    "    # network = pp.Network()\n",
    "    # for u,v in edge_list:\n",
    "    #     network.add_edge(u,v)\n",
    "    \n",
    "    # return network\n",
    "    return edge_list, m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from manage_experiments import *\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def logarithm_base(x, base):\n",
    "    return np.log(x)/np.log(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "def create_data_object_SBM(edges, labels, bip_block_p_pass_probs, theta, degree_p_pass_correction=True):\n",
    "\n",
    "    student_indices = [j for i,j in zip(labels, range(len(labels))) if i%2==0]\n",
    "    student_reindexed = {j:i for i,j in enumerate(student_indices)}\n",
    "    task_indices = [j for i,j in zip(labels, range(len(labels))) if i%2==1]\n",
    "    task_reindexed = {j:i for i,j in enumerate(task_indices)}\n",
    "    n_students = len(student_indices)\n",
    "    n_tasks = len(task_indices)\n",
    "\n",
    "    edge_reordered_reindexed = []\n",
    "    for edge in edges:\n",
    "        if edge[0] in student_indices:\n",
    "            e = (student_reindexed[edge[0]], task_reindexed[edge[1]])\n",
    "        else:\n",
    "            e = (student_reindexed[edge[1]], task_reindexed[edge[0]])\n",
    "        edge_reordered_reindexed.append(e)\n",
    "\n",
    "    # mapping groups to new indexing and node type separation\n",
    "    student_groups = np.zeros(n_students,dtype=int)\n",
    "    tasks_groups = np.zeros(n_tasks, dtype=int)\n",
    "\n",
    "    for old_ix, new_ix in student_reindexed.items():\n",
    "        student_groups[new_ix] = labels[old_ix]\n",
    "\n",
    "    for old_ix, new_ix in task_reindexed.items():\n",
    "        tasks_groups[new_ix] = labels[old_ix]\n",
    "\n",
    "\n",
    "    # Maiking students interact with tasks according to generated topology\n",
    "    # p_pass_in = .9\n",
    "    # p_pass_out = .1\n",
    "    y = []\n",
    "    theta_m = max(theta)\n",
    "    # tau = .0\n",
    "    # eta = .99\n",
    "    # s = - np.log((1 + eta) / eta) / theta_m\n",
    "    # print(s)\n",
    "    for s_ix, t_ix in edge_reordered_reindexed:\n",
    "        # if student_groups[s_ix] == tasks_groups[t_ix]:\n",
    "        #     p = p_pass_in\n",
    "        # else:\n",
    "        #     p = p_pass_out\n",
    "        p_pass = bip_block_p_pass_probs[student_groups[s_ix], tasks_groups[t_ix]]\n",
    "        # if degree_p_pass_correction:\n",
    "        #     # correction = logarithm_base(theta[s_ix], base)/logarithm_base(theta_m, base)#tau + (1 - tau)*sigmoid(s * theta[s_ix])\n",
    "        #     p_pass = p_pass * correction\n",
    "        y.append(np.random.binomial(1, p_pass))\n",
    "    \n",
    "\n",
    "    data  = HeteroData()\n",
    "\n",
    "    # Save node indices\n",
    "    data['student'].node_id = torch.arange(n_students)\n",
    "    data['item'].node_id = torch.arange(n_tasks)\n",
    "\n",
    "\n",
    "    # Add the node features\n",
    "    data['student'].x= torch.eye(n_students)\n",
    "    data['item'].x = torch.eye(n_tasks)\n",
    "\n",
    "    # Add the edge indices\n",
    "    data['student', 'responds', 'item'].edge_index = torch.from_numpy(np.array(edge_reordered_reindexed).T).to(torch.long)\n",
    "\n",
    "    #add the edge attrs\n",
    "    data['student', 'responds', 'item'].edge_attr = torch.tensor([1]*len(y)).to(torch.float).reshape(-1,1)\n",
    "\n",
    "    # Add the edge label\n",
    "    data['student', 'responds', 'item'].y = torch.from_numpy(np.array(y)).to(torch.long)\n",
    "\n",
    "    # We use T.ToUndirected() to add the reverse edges from subject to students \n",
    "    # in order to let GNN pass messages in both ways\n",
    "    data = T.ToUndirected()(data)\n",
    "    del data['item', 'rev_responds', 'student'].edge_attr  # Remove 'reverse' label.\n",
    "    del data['item', 'rev_responds', 'student'].y  # Remove 'reverse' label.\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_data_object_SBM(num_nodes, bip_block_probs, theta, bip_block_p_pass_probs, degree_p_pass_correction=False):\n",
    "    edges, labels = sample_dcsbm(num_nodes, bip_block_probs, theta) #generate_observations_tree_of_topics(n, n_students, n_tasks, tree_depth,branching_factor,p_ER,skill_mismatch_penalty)\n",
    "    data = create_data_object_SBM(edges, labels, bip_block_p_pass_probs, theta, degree_p_pass_correction)\n",
    "    # print(\"network density\",data.num_edges/(2*n_students*n_tasks))\n",
    "    return data\n",
    "\n",
    "\n",
    "# data_object = generate_data_object_SBM(num_nodes, bip_block_probs, theta)\n",
    "# data_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def line_plot_with_std(list_scores, parameters, fname):\n",
    "\n",
    "    # Convert the list of lists to a Pandas DataFrame\n",
    "    df = pd.DataFrame(list_scores)\n",
    "\n",
    "    # Calculate the mean and standard deviation of each row\n",
    "    mean = df.mean(axis=1)\n",
    "    std = df.std(axis=1)\n",
    "\n",
    "    # Create a figure and an axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the mean line\n",
    "    ax.plot(mean, color='blue', label='Mean')\n",
    "\n",
    "    # Plot the shaded region around the mean line\n",
    "    ax.fill_between(df.index, mean-std, mean+std, color='lightblue', alpha=0.5, label='Standard deviation')\n",
    "\n",
    "    # Add some labels and a legend\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Value')\n",
    "    # ax.legend()\n",
    "\n",
    "    # change x-ticks to start from \n",
    "    n_tasks_per_student_list = list(range(\n",
    "        parameters['min_n_tasks_per_student'],\n",
    "        parameters['max_n_tasks_per_student'],\n",
    "        parameters['step_n_tasks_per_student']))\n",
    "    ax.set_xticks(range(len(n_tasks_per_student_list)), n_tasks_per_student_list, rotation=45)\n",
    "\n",
    "    # Show and save the plot\n",
    "    plt.grid()\n",
    "    plt.xlabel('Number of tasks per student')\n",
    "    plt.ylabel('Balanced Accuracy')\n",
    "    fnameplot = fname.replace('results', 'pdf')\n",
    "    plt.savefig(fnameplot)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_confusion_matrix(json_content, fname):\n",
    "    fig, ax = plt.subplots(ncols = 1, figsize=(8,6))\n",
    "    \n",
    "    plt.imshow(json_content[\"Confusion_2_test\"], interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "\n",
    "    classes = [0, 1]\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    part1, part2 = fname.split('.')\n",
    "    part1 = part1 + '_consfusion_matrix'\n",
    "    fname = '.'.join([part1, part2])\n",
    "    fnameplot = fname.replace('results', 'pdf')\n",
    "    plt.savefig(fnameplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.utils import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "def create_filename_results(parameters,fold):\n",
    "    file_name = ' '.join([f\"{str(key)}-{str(value)}\" for key, value in parameters.items() if key not in ['bip_block_probs','theta','bip_block_p_pass_probs', 'degree_p_pass_correction', 'base']])\n",
    "    file_name += ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(4))\n",
    "    file_name.replace(\".\",\"\")\n",
    "    file_name+=\".results\"\n",
    "    return os.path.join(fold, file_name)\n",
    "\n",
    "\n",
    "#n_students_per_task_list = [2**i for i in n_students_per_task_list]\n",
    "\n",
    "import json\n",
    "def synthetic_density_effect_runs(data, parameters,parameters_density, folder = 'infographic_plots'):\n",
    "    parameters = {**parameters, **parameters_density}\n",
    "    scores = []\n",
    "    scores_roc = []\n",
    "    list_scores = []\n",
    "    list_scores_roc = []\n",
    "    for _ in range(parameters[\"n_runs\"]):\n",
    "        with io.capture_output() as captured:\n",
    "            \n",
    "            cv_out = perform_cross_validation(data, parameters, save_embeddings=True)\n",
    "\n",
    "            score = 0\n",
    "            roc = 0\n",
    "            inner_list_scores = []\n",
    "            inner_list_scores_roc = []\n",
    "            for fold_n in range(parameters[\"n_splits\"]):\n",
    "                score += cv_out[f\"Balanced Accuracy_{fold_n}_test\"]\n",
    "                roc += cv_out[f\"AUC_{fold_n}_test\"]\n",
    "                inner_list_scores.extend([cv_out[f\"Balanced Accuracy_{fold_n}_test\"]])\n",
    "                inner_list_scores_roc.extend([cv_out[f\"AUC_{fold_n}_test\"]])\n",
    "            score = score / parameters[\"n_splits\"]\n",
    "            roc = roc / parameters[\"n_splits\"]\n",
    "            scores.append(score)\n",
    "            scores_roc.append(roc)\n",
    "\n",
    "            list_scores.append(inner_list_scores)\n",
    "            list_scores_roc.append(inner_list_scores_roc)\n",
    "\n",
    "\n",
    "    # density = data.num_edges/(2*parameters[\"n_students\"]*parameters[\"n_tasks\"]) \n",
    "    res_dict = dict(zip(list(range(parameters[\"n_runs\"])),scores))\n",
    "    res_dict_roc = dict(zip(list(range(parameters[\"n_runs\"])),scores_roc))\n",
    "    output_dict = {\n",
    "        **parameters,\n",
    "        # \"density\":density,\n",
    "        \"res_dict\":res_dict,\n",
    "        \"res_dict_roc\":res_dict_roc,\n",
    "        \"list_scores\":dict(zip(list(range(parameters[\"n_runs\"])),list_scores)),\n",
    "        \"list_scores_roc\":dict(zip(list(range(parameters[\"n_runs\"])),list_scores_roc)),\n",
    "    }\n",
    "    output_dict['bip_block_probs'] = output_dict['bip_block_probs'].tolist()\n",
    "    output_dict['theta'] = output_dict['theta'].tolist()\n",
    "    output_dict['bip_block_p_pass_probs'] = output_dict['bip_block_p_pass_probs'].tolist()\n",
    "    fname = create_filename_results(parameters_density, folder)\n",
    "    # print the types of items\n",
    "    # print({i: type(output_dict[i]) for i,j in output_dict.items()})\n",
    "    if 'embedding_0' in cv_out:\n",
    "        output_dict['embeddings'] = {k: v.tolist() for k, v in cv_out['embedding_0'].items()}\n",
    "    with open(fname,'w') as f:\n",
    "        json.dump(output_dict, f, skipkeys=True)\n",
    "    \n",
    "    # save results\n",
    "    # print(density)\n",
    "    print([np.mean(s) for s in list_scores])\n",
    "    return output_dict\n",
    "    # line_plot_with_std(list_scores=list_scores, parameters= parameters, fname=fname)\n",
    "\n",
    "    # # save embeddings\n",
    "    # visualise_embeddings(output_dict, fname=fname)\n",
    "\n",
    "    # visualise_embeddings_concatenated(output_dict, data['student', 'responds', 'item'].edge_index, data['student', 'item'].y, fname = fname) # plot with indices of train-test split\n",
    "    \n",
    "    # # plot with indices of train-test split\n",
    "    \n",
    "\n",
    "\n",
    "    # visualise_confusion_matrix(cv_out, fname=fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"hidden_dims\": None,\n",
    "    'model_type': None,\n",
    "    \"df_name\": \"synthetic.salamoia\",\n",
    "    \"method\": \"EdgeClassifier\",\n",
    "    \"epochs\": 2000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"weight_decay\": 0,\n",
    "    \"dropout\": 0.0,\n",
    "    \"early_stopping\": 250,\n",
    "    \"n_splits\": 3,\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"done\": False,\n",
    "    \"batch_size\":64*4096,\n",
    "    #\n",
    "    }\n",
    "\n",
    "# n_groups = 3\n",
    "# # probability of connections between students and tasks\n",
    "# dict_p_stud_to_task = {}\n",
    "# dict_p_stud_to_task[0] = [1,1/(n_groups - 1),1/(n_groups - 1)]\n",
    "# dict_p_stud_to_task[1] = [1/(n_groups - 1),1,1/(n_groups - 1)]\n",
    "# dict_p_stud_to_task[2] = [1/(n_groups - 1),1/(n_groups - 1),1]\n",
    "# bip_block_probs = generate_bipartite_block_probs(dict_p_stud_to_task)\n",
    "# # probability of passing a task given the student and the task\n",
    "# dict_p_pass_stud_to_task = {}\n",
    "# dict_p_pass_stud_to_task[0] = [.8,.1,.05]\n",
    "# dict_p_pass_stud_to_task[1] = [.2,.7,.1]\n",
    "# dict_p_pass_stud_to_task[2] = [.1,.3,.6]\n",
    "# bip_block_p_pass_probs = generate_bipartite_block_probs(dict_p_stud_to_task)\n",
    "\n",
    "# theta = np.random.randint(1,num_nodes//3,num_nodes)\n",
    "\n",
    "parameters_density = {\n",
    "    'n_runs': 1,\n",
    "    'num_nodes':1000,\n",
    "    'bip_block_probs':None,\n",
    "    'bip_block_p_pass_probs':None,\n",
    "    'theta':None,\n",
    "    'n_groups':None,\n",
    "    'theta_type':None,\n",
    "    'p_pass_out': None,\n",
    "    'p_pass_in': None,\t\n",
    "    'degree_p_pass_correction': True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: IRT hidden_dims: 1 n_groups: 3 theta: uniform mul_p_connection: 1 p_pass_out: 0.1\n",
      "y counts tensor([12493, 15079])\n",
      "[0.6649945519281255]\n",
      "Model type: IRT hidden_dims: 3 n_groups: 3 theta: uniform mul_p_connection: 1 p_pass_out: 0.1\n",
      "y counts tensor([12493, 15079])\n",
      "[0.9232055645772853]\n",
      "Model type: IRT hidden_dims: 5 n_groups: 3 theta: uniform mul_p_connection: 1 p_pass_out: 0.1\n",
      "y counts tensor([12493, 15079])\n",
      "[0.9050605514489692]\n",
      "Model type: GNN hidden_dims: [16, 16, 8] n_groups: 3 theta: uniform mul_p_connection: 1 p_pass_out: 0.1\n",
      "y counts tensor([12493, 15079])\n"
     ]
    }
   ],
   "source": [
    "# NB: having only the leaves might make it easier for IRT since it just creates separated bins (i.e., the leaves) and easy tasks (the ones sampled close to the root node)\n",
    "#  Instead, with a GNN we want to leverage the ...ability to encode the underlying tree structure (HOW might the GNN do it, specifically?)\n",
    "#  (np.random.normal(parameters_density[\"num_nodes\"]/n_groups, std, parameters_density[\"num_nodes\"]), \"gaussian\"), \n",
    "std = parameters_density[\"num_nodes\"]/100\n",
    "for correction in [False]: \n",
    "    parameters_density['degree_p_pass_correction'] = correction\n",
    "    for n_groups in [3, 4, 5]: #3,4,5parameters_density\n",
    "        parameters_density['n_groups'] = n_groups\n",
    "        for theta in [ (np.random.randint(1,parameters_density[\"num_nodes\"]//n_groups,parameters_density[\"num_nodes\"]), \"uniform\"), (np.random.normal(parameters_density[\"num_nodes\"]/n_groups, std, parameters_density[\"num_nodes\"]), \"gaussian\")]:#, (np.random.exponential(100,parameters_density[\"num_nodes\"]), \"exponential\")]:\n",
    "            #print(theta)\n",
    "            parameters_density['theta'] = theta[0]\n",
    "            parameters_density['theta_type'] = theta[1]\n",
    "            for mul_p_connection in [1]:\n",
    "                parameters_density['mul_p_connection'] = mul_p_connection\n",
    "                dict_p_stud_to_task = {}\n",
    "                for i in range(n_groups):\n",
    "                    dict_p_stud_to_task[i] = [mul_p_connection/(n_groups-1)]*n_groups\n",
    "                    dict_p_stud_to_task[i][i] = 1\n",
    "                bip_block_probs = generate_bipartite_block_probs(dict_p_stud_to_task)\n",
    "\n",
    "                parameters_density['bip_block_probs'] = bip_block_probs\n",
    "                parameters_density['p_pass_in'] = 0.7\n",
    "                for p_pass_out in [.1, .3, .5]: \n",
    "                    dict_p_pass_stud_to_task = {}\n",
    "                    for i in range(n_groups):\n",
    "                        dict_p_pass_stud_to_task[i] = [p_pass_out]*n_groups\n",
    "                        dict_p_pass_stud_to_task[i][i] = 1\n",
    "                    parameters_density['p_pass_out'] = p_pass_out\n",
    "                    bip_block_p_pass_probs = generate_bipartite_block_probs(dict_p_pass_stud_to_task)\n",
    "\n",
    "                    parameters_density['bip_block_p_pass_probs'] = bip_block_p_pass_probs\n",
    "\n",
    "                    parameters = {**parameters, **parameters_density}\n",
    "\n",
    "                    \n",
    "                    data = generate_data_object_SBM(\n",
    "                            num_nodes= parameters[\"num_nodes\"],\n",
    "                            bip_block_probs = parameters[\"bip_block_probs\"],\n",
    "                            theta = parameters[\"theta\"],\n",
    "                            bip_block_p_pass_probs = parameters[\"bip_block_p_pass_probs\"],\n",
    "                            # base = parameters[\"base\"],\n",
    "                            degree_p_pass_correction=parameters[\"degree_p_pass_correction\"]\n",
    "                    )\n",
    "                    \n",
    "                    for model_type,hidden_dims in [\n",
    "                        (\"IRT\", 1),\n",
    "                        (\"IRT\", 3),\n",
    "                        (\"IRT\", 5),\n",
    "                        (\"GNN\", [\n",
    "                            16,\n",
    "                            16,\n",
    "                            8])]:\n",
    "                        parameters[\"model_type\"] = model_type\n",
    "                        parameters[\"hidden_dims\"] = hidden_dims\n",
    "                        print('Model type:', model_type, 'hidden_dims:', hidden_dims, 'n_groups:', n_groups, 'theta:', theta[1], 'mul_p_connection:', mul_p_connection, 'p_pass_out:', p_pass_out)#, 'base:', base)\n",
    "                        print('y counts', torch.bincount(data['student', 'responds', 'item'].y))\n",
    "                        output_dict = synthetic_density_effect_runs(data, parameters=parameters,\n",
    "                                                    parameters_density=parameters_density,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_embeddings_concatenated(json_content, edge_label_index, y, fname, test_indices = None):\n",
    "\n",
    "    def plot_tsne(embeddings, targets, title, ax):\n",
    "        # Perform t-SNE dimensionality reduction\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        embeddings_2d = tsne.fit_transform(np.array(embeddings))\n",
    "\n",
    "        # Plotting the scatter plot\n",
    "        ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=targets, cmap='viridis', alpha=0.5)\n",
    "        ax.set_title(title)\n",
    "        # ax.set_xlabel('t-SNE Dimension 1')\n",
    "        # ax.set_ylabel('t-SNE Dimension 2')\n",
    "\n",
    "    embeddings =  {k: np.array(v) for k, v in json_content[\"embeddings\"].items()}\n",
    "    embeddings_students = torch.tensor(embeddings['student'])\n",
    "    embeddings_items = torch.tensor(embeddings[\"item\"])\n",
    "    # student_ability = json_content['student_ability']\n",
    "    # item_difficulty = json_content[\"item_difficulty\"]\n",
    "\n",
    "    edge_feat_student = embeddings_students[edge_label_index[0]]\n",
    "    edge_feat_item = embeddings_items[edge_label_index[1]]\n",
    "\n",
    "    concat_embeddings = torch.cat([edge_feat_student, edge_feat_item], dim=1)\n",
    "\n",
    "    # Creating subplots for t-SNE visualization\n",
    "    fig, ax = plt.subplots(ncols = 1, figsize=(8,6))\n",
    "\n",
    "    if test_indices is None:\n",
    "        plot_tsne(concat_embeddings, y, 'Edge Representations', ax)\n",
    "    else:\n",
    "        plot_tsne(concat_embeddings, [1 if i in test_indices else 0 for i in range(edge_label_index.shape[1])], 'Edge Representations', ax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    part1, part2 = fname.split('.')\n",
    "    part1 = part1 + '_scatter_concatenated'\n",
    "    fname = '.'.join([part1, part2])\n",
    "    fnameplot = fname.replace('results', 'pdf')\n",
    "    plt.savefig(fnameplot)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'embeddings' in output_dict:\n",
    "    visualise_embeddings_concatenated(output_dict, data['student', 'responds', 'item'].edge_index, data['student', 'item'].y, fname = 'infographic_plots/test.results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def read_json_files(directory):\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".results\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "                data.append(json_data)\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = read_json_files('infographic_plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hidden_dims.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[df.hidden_dims.apply(lambda x: isinstance(x, list)), 'hidden_dims'] = 1\n",
    "df_1 = df.loc[df.model_type == 'GNN', :].copy() \n",
    "df_3 = df.loc[df.model_type == 'GNN', :].copy() \n",
    "df_5 = df.loc[df.model_type == 'GNN', :].copy() \n",
    "df_1.hidden_dims = 1\n",
    "df_3.hidden_dims = 3\n",
    "df_5.hidden_dims = 5\n",
    "\n",
    "df = pd.concat((df.loc[df.model_type == 'IRT'], df_1, df_3, df_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hidden_dims.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['list_scores_latex'] = df['list_scores'].apply(lambda x: f\"{np.mean([np.mean(v)*100 for v in x.values()]):.2f} ± {np.std([np.mean(v)*100 for v in x.values()]):.2f}\")\n",
    "df['list_scores_latex'] = df['list_scores'].apply(lambda x: f\"{np.mean([v for v in x.values()])*100:.2f} ± {np.std([v for v in x.values()])*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\n",
    "    'hidden_dims',\n",
    "    'p_pass_in', \n",
    "    'p_pass_out', \n",
    "    # 'bip_block_p_pass_probs', \n",
    "    'theta_type', \n",
    "    'n_groups', \n",
    "    'mul_p_connection',\n",
    "    'model_type',\n",
    "    'degree_p_pass_correction',\n",
    "    # 'base'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(group_cols).agg({\n",
    "    'list_scores': lambda x: x.apply(lambda y: np.mean([v for v in y.values()])),\n",
    "    }).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_list = []\n",
    "mean_list = []\n",
    "for dict_values in df.list_scores.values:\n",
    "    std_list.append(np.std(dict_values['0']))\n",
    "    mean_list.append(np.mean(dict_values['0']))\n",
    "\n",
    "df['std'] = std_list\n",
    "df['mean'] = mean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_selector(df, cols, values):\n",
    "    mask = np.full(len(df), True)\n",
    "    for col, value in zip(cols, values):\n",
    "        mask = mask & (df[col] == value)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped['Mean'] = grouped['list_scores'].apply(lambda x: round(np.mean(x),2))\n",
    "grouped['Std'] = grouped['list_scores'].apply(lambda x: round(np.std(x),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for group, df_group in df.groupby(['n_groups', 'theta_type', 'mul_p_connection', 'p_pass_out', 'degree_p_pass_correction', 'hidden_dims']):\n",
    "    try:\n",
    "        gain = (df_group[df_group['model_type'] == 'GNN']['mean'].values[0] - df_group[df_group['model_type'] == 'IRT']['mean'].values[0]) / df_group[df_group['model_type'] == 'IRT']['mean'].values[0]*100\n",
    "        std = np.sqrt((df_group[df_group['model_type'] == 'GNN']['std'].values[0])**2 + (df_group[df_group['model_type'] == 'IRT']['std'].values[0])**2)\n",
    "        gnn_mean = (df_group[df_group['model_type'] == 'GNN']['mean'].values[0])*100\n",
    "        gnn_std = (df_group[df_group['model_type'] == 'GNN']['std'].values[0])*100\n",
    "        irt_mean = (df_group[df_group['model_type'] == 'IRT']['mean'].values[0])*100\n",
    "        irt_std = (df_group[df_group['model_type'] == 'IRT']['std'].values[0])*100\n",
    "        df_list.append(pd.DataFrame({\n",
    "            'n_groups': [group[0]],\n",
    "            'theta_type': [group[1]],\n",
    "            'mul_p_connection': [group[2]],\n",
    "            'p_pass_out': [group[3]],\n",
    "            # 'base': [group[4]],\n",
    "            'degree_p_pass_correction': [group[4]],\n",
    "            'hidden_dims': [group[5]],\n",
    "            'gain': [gain],\n",
    "            'std': [std *100],\n",
    "            'gnn_mean': [gnn_mean],\n",
    "            'irt_mean': [irt_mean],\n",
    "            'gnn_std': [gnn_std],\n",
    "            'irt_std': [irt_std],\n",
    "            'bottom': df_group[df_group['model_type'] == 'IRT']['mean'].values[0]*100\t\n",
    "        }))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "mul_p_connection = 1\n",
    "theta_types = ['gaussian', 'uniform']\n",
    "n_groups = [3, 4, 5]\n",
    "base = 10\n",
    "degree_p_pass_correction = False\n",
    "hidden_dims = 5\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    len(theta_types),  # Swap the number of rows and columns\n",
    "    len(n_groups),\n",
    "    figsize=(7, 5),\n",
    "    sharey=True\n",
    "    )\n",
    "for theta_type in theta_types: \n",
    "    for ng in n_groups:\n",
    "        row = theta_types.index(theta_type)  # Swap the roles of rows and columns\n",
    "        col = n_groups.index(ng)\n",
    "        sub_new_df = new_df[\n",
    "            (new_df['theta_type'] == theta_type) & \n",
    "            (new_df['n_groups'] == ng) & \n",
    "            (new_df['mul_p_connection'] == mul_p_connection) &\n",
    "            (new_df['degree_p_pass_correction'] == degree_p_pass_correction) &\n",
    "            (new_df['hidden_dims'] == hidden_dims)\n",
    "            # (new_df['base'] == base) &\n",
    "            ]\n",
    "        \n",
    "        axes[row,col].grid()\n",
    "        sub_new_df = sub_new_df.copy()\n",
    "        sub_new_df['color'] = sub_new_df['gain'].apply(lambda x: 'blue' if x>0 else 'red')\n",
    "        axes[row, col].plot(\n",
    "            sub_new_df['p_pass_out'],\n",
    "            sub_new_df['gnn_mean'],\n",
    "            color='blue'\n",
    "            )\n",
    "        axes[row, col].errorbar(\n",
    "            sub_new_df['p_pass_out'],\n",
    "            sub_new_df['gnn_mean'],\n",
    "            yerr=sub_new_df['gnn_std'],\n",
    "            color='blue'\n",
    "            )\n",
    "        axes[row, col].plot(\n",
    "            sub_new_df['p_pass_out'],\n",
    "            sub_new_df['irt_mean'],\n",
    "            color='red'\n",
    "            )\n",
    "        axes[row, col].errorbar(\n",
    "            sub_new_df['p_pass_out'],\n",
    "            sub_new_df['irt_mean'],\n",
    "            yerr=sub_new_df['irt_std'],\n",
    "            color='red'\n",
    "            )\n",
    "\n",
    "        axes[row,col].set_title(f\"{theta_type.capitalize()}, $g=${ng}\")# {base} base\")\n",
    "        if col == 0:  # Change this condition to adjust y-axis labels\n",
    "            axes[row,col].set_ylabel('Balanced Accuracy (%)')\n",
    "\n",
    "        if row == 1:  # Change this condition to adjust x-axis labels\n",
    "            axes[row,col].set_xlabel('$p_{out}$')\n",
    "\n",
    "        axes[row,col].set_xticks(sub_new_df['p_pass_out'])\n",
    "        axes[row,col].set_yticks(np.arange(40, 100, 10))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'plots_synthetic/Mul_p_connection_{mul_p_connection}_uncorrected_{hidden_dims}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_p_connection = 1\n",
    "\n",
    "theta_types = ['gaussian', 'uniform']\n",
    "n_groups = [3, 4, 5]\n",
    "base = 10\n",
    "degree_p_pass_correction = True\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    len(theta_types),  # Swap the number of rows and columns\n",
    "    len(n_groups),\n",
    "    figsize=(7, 5),\n",
    "    sharey=True\n",
    "    )\n",
    "for theta_type in theta_types: \n",
    "    for ng in n_groups:\n",
    "        row = theta_types.index(theta_type)  # Swap the roles of rows and columns\n",
    "        col = n_groups.index(ng)\n",
    "        sub_new_df = new_df[\n",
    "            (new_df['theta_type'] == theta_type) & \n",
    "            (new_df['n_groups'] == ng) & \n",
    "            (new_df['mul_p_connection'] == mul_p_connection) &\n",
    "            (new_df['degree_p_pass_correction'] == degree_p_pass_correction)\n",
    "            # (new_df['base'] == base) &\n",
    "            ]\n",
    "        \n",
    "        axes[row,col].grid()\n",
    "        sub_new_df = sub_new_df.copy()\n",
    "        sub_new_df['color'] = sub_new_df['gain'].apply(lambda x: 'blue' if x>0 else 'red')\n",
    "        axes[row, col].bar(\n",
    "            x=sub_new_df['p_pass_out'],\n",
    "            height=sub_new_df['gain'],\n",
    "            yerr=sub_new_df['std'],\n",
    "            color=sub_new_df['color'].values.tolist(),\n",
    "            width=0.1,\n",
    "            align='center',\n",
    "            bottom=sub_new_df['bottom'],\n",
    "            zorder = 2,\n",
    "            capsize=2\n",
    "            )\n",
    "        axes[row,col].set_title(f\"{theta_type.capitalize()}, $g=${ng}\")# {base} base\")\n",
    "        if col == 0:  # Change this condition to adjust y-axis labels\n",
    "            axes[row,col].set_ylabel('Balanced Accuracy (%)')\n",
    "\n",
    "        if row == 1:  # Change this condition to adjust x-axis labels\n",
    "            axes[row,col].set_xlabel('$p_{out}$')\n",
    "\n",
    "        axes[row,col].set_xticks(sub_new_df['p_pass_out'])\n",
    "        axes[row,col].set_yticks(np.arange(40, 100, 10))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'plots_synthetic/Mul_p_connection_{mul_p_connection}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils.convert import to_networkx\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = to_networkx(data)\n",
    "G.remove_nodes_from(list(nx.isolates(G)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 10))\n",
    "import networkx as nx\n",
    "nx.draw_networkx(G, pos=nx.bipartite_layout(G, data['student']['node_id'].tolist()), node_size=5, alpha=0.1, with_labels=False, width=0.1, arrows=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['model_type'] == 'GNN') & (df['mul_p_connection'] == 2/3) & (df['p_pass_out'] == 0.1) & (df['theta_type'] == 'lognormal') & (df['n_groups'] == 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['model_type'] == 'IRT') & (df['mul_p_connection'] == 2/3) & (df['p_pass_out'] == 0.1) & (df['theta_type'] == 'lognormal') & (df['n_groups'] == 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_gaussian = new_df[new_df['theta_type'] == 'gaussian']\n",
    "new_df_lognormal = new_df[new_df['theta_type'] == 'lognormal']\n",
    "new_df_uniform = new_df[new_df['theta_type'] == 'uniform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 15))\n",
    "\n",
    "sns.barplot(x='n_groups', y='gain', data=new_df_gaussian, ax=axes[0,0])\n",
    "axes[0,0].set_title('n_groups')\n",
    "\n",
    "sns.barplot(x='mul_p_connection', y='gain', data=new_df_gaussian, ax=axes[1,0])\n",
    "axes[1,0].set_title('mul_p_connection')\n",
    "\n",
    "sns.barplot(x='p_pass_out', y='gain', data=new_df_gaussian, ax=axes[2,0])\n",
    "axes[2,0].set_title('p_pass_out')\n",
    "\n",
    "\n",
    "sns.barplot(x='n_groups', y='gain', data=new_df_uniform, ax=axes[0,1])\n",
    "axes[0,1].set_title('n_groups')\n",
    "\n",
    "sns.barplot(x='mul_p_connection', y='gain', data=new_df_uniform, ax=axes[1,1])\n",
    "axes[1,1].set_title('mul_p_connection')\n",
    "\n",
    "sns.barplot(x='p_pass_out', y='gain', data=new_df_uniform, ax=axes[2,1])\n",
    "axes[2,1].set_title('p_pass_out')\n",
    "\n",
    "\n",
    "sns.barplot(x='n_groups', y='gain', data=new_df_lognormal, ax=axes[0,2])\n",
    "axes[0,2].set_title('n_groups')\n",
    "\n",
    "sns.barplot(x='mul_p_connection', y='gain', data=new_df_lognormal, ax=axes[1,2])\n",
    "axes[1,2].set_title('mul_p_connection')\n",
    "\n",
    "sns.barplot(x='p_pass_out', y='gain', data=new_df_lognormal, ax=axes[2,2])\n",
    "axes[2,2].set_title('p_pass_out')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes=500\n",
    "mu = 20\n",
    "plt.hist(np.random.lognormal(mean=np.log(mu),sigma=np.log(2),size=n_nodes), bins = 100)\n",
    "plt.vlines(x=mu,ymin=0,ymax=15,colors=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_object_SBM(num_nodes, bip_block_probs, theta, bip_block_p_pass_probs):\n",
    "    edges, labels = sample_dcsbm(num_nodes, bip_block_probs, theta) #generate_observations_tree_of_topics(n, n_students, n_tasks, tree_depth,branching_factor,p_ER,skill_mismatch_penalty)\n",
    "    data = create_data_object_SBM(edges, labels, bip_block_p_pass_probs, theta)\n",
    "    # print(\"network density\",data.num_edges/(2*n_students*n_tasks))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 500\n",
    "theta = np.random.lognormal(mean=np.log(20),sigma=np.log(2),size=num_nodes)\n",
    "n_groups = 3\n",
    "mul_p_connection = 1/2\n",
    "dict_p_stud_to_task = {}\n",
    "for i in range(n_groups):\n",
    "    dict_p_stud_to_task[i] = [mul_p_connection/(n_groups-1)]*n_groups\n",
    "    dict_p_stud_to_task[i][i] = 1\n",
    "bip_block_probs = generate_bipartite_block_probs(dict_p_stud_to_task)\n",
    "\n",
    "p_pass_out = .1\n",
    "dict_p_pass_stud_to_task = {}\n",
    "for i in range(n_groups):\n",
    "    dict_p_pass_stud_to_task[i] = [p_pass_out]*n_groups\n",
    "    dict_p_pass_stud_to_task[i][i] = 1\n",
    "parameters_density['p_pass_out'] = p_pass_out\n",
    "bip_block_p_pass_probs = generate_bipartite_block_probs(dict_p_pass_stud_to_task)\n",
    "\n",
    "data = generate_data_object_SBM(num_nodes, bip_block_probs, theta, bip_block_p_pass_probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_val in grouped.p_pass_out.unique():\n",
    "    print(p_val,grouped[grouped.p_pass_out == p_val].n_groups.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_val in grouped.p_pass_out.unique():\n",
    "    print(grouped[grouped.p_pass_out == p_val].mul_p_connection.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped[grouped.p_pass_out == p_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, alpha):\n",
    "    return 1/(1+np.exp(-x/alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-9,10)\n",
    "plt.plot(x, [sigmoid(xi,0) for xi in x], label=\"alpha = 0\")\n",
    "plt.plot(x, [sigmoid(xi,1) for xi in x], label=\"alpha = 1\")\n",
    "plt.plot(x, [sigmoid(xi,2) for xi in x], label=\"alpha = 2\")\n",
    "plt.plot(x, [sigmoid(xi,3) for xi in x], label=\"alpha = 3\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathpy as pp\n",
    "from collections import defaultdict \n",
    "import copy\n",
    "# tree\n",
    "root = \"*\"\n",
    "max_depth = 3\n",
    "nodes_at_layer = defaultdict(list)\n",
    "nodes_at_layer[0].append(root)\n",
    "start_nodes = [\"0\",\"1\"]\n",
    "nodes_at_layer[1].extend(start_nodes)\n",
    "for depth in range(2,max_depth+1):\n",
    "    # nodes_this_layer = []\n",
    "    # nodes_prev_layer = copy.deepcopy(nodes_at_layer[depth-1])\n",
    "    for node in nodes_at_layer[depth-1]:\n",
    "        for base_node in start_nodes:\n",
    "            nodes_at_layer[depth].append(node+base_node)\n",
    "\n",
    "\n",
    "nodes_at_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = pp.Network(directed = True)\n",
    "for depth in range(max(nodes_at_layer)):\n",
    "    if depth == 0:\n",
    "        for node in nodes_at_layer[1]:\n",
    "            n.add_edge(root,node)\n",
    "    else:\n",
    "        for node_parent in nodes_at_layer[depth]:\n",
    "            for node_child in nodes_at_layer[depth+1]:\n",
    "                if node_child[:-1]==node_parent:\n",
    "                    n.add_edge(node_parent,node_child)\n",
    "print(n.ncount())\n",
    "pp.visualisation.plot(n, width = 800,height = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(n.nodes)\n",
    "n_items = 100\n",
    "# NB: randomly sampling nodes will disproportionately give leaves, i.e., difficult items.\n",
    "item_positions = np.random.choice(nodes,size=n_items)\n",
    "dict_item_positions = dict(zip(range(n_items),item_positions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_skills(n, study_hours,root=\"*\"):\n",
    "    # gives skills to students\n",
    "    # we start by taking one random path from root to leaf for each student\n",
    "    # extensions goes in the direction of using personalized pagerank from the root node and would include: \n",
    "    #   study hours (similar to ability, encodes number of transitions taken by the students rw)\n",
    "    #   restart probability tau at any node\n",
    "    #   once an end node is reached, student restards from prev visited node (or root) with prob 1\n",
    "    #\n",
    "    # for now just enough to reach one leaf\n",
    "    T = n.transition_matrix().todense().T\n",
    "    root_index = n.node_to_name_map()[root]\n",
    "    ix_to_node = {v:k for k,v in n.node_to_name_map().items()}\n",
    "    last_node_ix = root_index\n",
    "    for _ in range(study_hours):\n",
    "        last_node_ix = np.random.choice(range(n.ncount()),p=T[last_node_ix,:].tolist()[0])\n",
    "    return ix_to_node[last_node_ix]\n",
    "\n",
    "\n",
    "n_students = 100\n",
    "dict_students_abilities = {stud_id:[] for stud_id in range(n_students)}\n",
    "for stud_id in dict_students_abilities:\n",
    "    # append, but potentially extend cause students can get skills in different branches\n",
    "    dict_students_abilities[stud_id].append(assign_skills(n,depth+1))\n",
    "# dict_students_abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# students interact randomly with items (erdos renyi). Student pass items if they have traversed the topic/tree-node in their learning path\n",
    "p_ER = .3\n",
    "edges = []\n",
    "labels = []\n",
    "for stud_id in dict_students_abilities:\n",
    "    for item_id in dict_item_positions:\n",
    "        if np.random.rand()<p_ER:\n",
    "            edges.append((stud_id,item_id))\n",
    "            # deciding if item is passed\n",
    "            # could be made more nuanced checking how much of the path to the item_topic has been traversed by the student\n",
    "            item_topic = dict_item_positions[item_id] \n",
    "            student_skills = dict_students_abilities[stud_id]\n",
    "            if any([item_topic==skill[:len(item_topic)] for skill in student_skills if len(skill)>=len(item_topic)]):\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "print(sum(labels)/len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATING NODE POSITIONS AND PLOTTING THEM \n",
    "# NB: ALSO SETTING RADIUS USED BELOW\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "dimension = 2\n",
    "n_students = 270\n",
    "n_items = 750\n",
    "#\n",
    "student_geom = np.random.rand(n_students,dimension)\n",
    "student_ids = range(n_students)\n",
    "items_geom = np.random.rand(n_items,dimension)\n",
    "items_id = range(n_students, n_students+n_items)\n",
    "#\n",
    "radius = .1\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(student_geom[:,0],student_geom[:,1], label = \"student\")\n",
    "ax.scatter(items_geom[:,0],items_geom[:,1], label = \"item\")\n",
    "plt.xlabel(\"Student ability\")\n",
    "plt.ylabel(\"Item difficulty\")\n",
    "circle = Circle([.5,.5], radius, fill=False, edgecolor='red')\n",
    "ax.add_patch(circle)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATING NODE POSITIONS AND PLOTTING THEM \n",
    "# NB: ALSO SETTING RADIUS USED BELOW\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "dimension = 2\n",
    "n_students = 270\n",
    "n_items = 750\n",
    "#\n",
    "student_geom = np.random.rand(n_students,dimension)\n",
    "student_ids = range(n_students)\n",
    "items_geom = np.random.rand(n_items,dimension)\n",
    "items_id = range(n_students, n_students+n_items)\n",
    "#\n",
    "radius = .1\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(student_geom[:,0],[0]*len(student_geom[:,1]), label = \"student\")\n",
    "ax.scatter([0]*len(items_geom[:,0]),items_geom[:,1], label = \"item\")\n",
    "plt.xlabel(\"Student ability\")\n",
    "plt.ylabel(\"Item difficulty\")\n",
    "# circle = Circle([.5,.5], radius, fill=False, edgecolor='red')\n",
    "# ax.add_patch(circle)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_students = 100\n",
    "n_items = 150\n",
    "\n",
    "list_a = [np.random.randint(50)/50 for _ in range(n_students)]\n",
    "list_i = [np.random.randint(50)/50 for _ in range(n_items)]\n",
    "\n",
    "edges = []\n",
    "for i in range(n_students):\n",
    "    # ability student i\n",
    "    a_i = list_a[i]\n",
    "    for j in range(n_items):\n",
    "        # difficulty item j\n",
    "        d_j = list_i[j]\n",
    "        # delta gives how appropriate taks diff is for student ability\n",
    "        delta = abs(a_i - d_j)\n",
    "        # probability of interaction prop to inverse of delta: \n",
    "        p_i_j = 1/(2+delta)**4\n",
    "        assert p_i_j <= 1\n",
    "        if np.random.rand()<p_i_j:\n",
    "            edges.append((i,i+j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathpy as pp\n",
    "n = pp.Network(directed=True)\n",
    "for (u,v) in edges:\n",
    "    n.add_edge(u,v)\n",
    "pp.visualisation.plot(n, width =800,height = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBTAINING PROBABILITY (AND PLOTTING PATTERNS FOR ON EDGES -- POSITION TAKING AVERAGE BETWEEN NODES INVOLVED IN THE INTERACTIONS)\n",
    "label_probs = []\n",
    "list_edge_points = []\n",
    "for i in range(n_students):\n",
    "    for j in range(n_items):\n",
    "        if np.linalg.norm(student_geom[i]-items_geom[j]) < radius:\n",
    "            # first coordinate used as student ability \n",
    "            # second coordinate used as item difficulty\n",
    "            x = student_geom[i][0]-items_geom[j][1]\n",
    "            label_probs.append(1/(1+np.exp(-x)))\n",
    "            list_edge_points.append((student_geom[i]+items_geom[j])/2)\n",
    "\n",
    "list_edge_points = np.array(list_edge_points)\n",
    "plt.xlabel(\"Student ability\")\n",
    "plt.ylabel(\"Item difficulty\")\n",
    "plt.scatter(list_edge_points[:,0],list_edge_points[:,1], c=label_probs)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # CREATING LIST OF EDGES\n",
    "# # import pathpy as pp\n",
    "# import numpy as np\n",
    "# n = pp.Network()\n",
    "# edges = []\n",
    "# for i in range(n_students):\n",
    "#     for j in range(n_items):\n",
    "#         if np.linalg.norm(student_geom[i]-items_geom[j]) < radius:\n",
    "#             edges.append((i,i+j))\n",
    "#             n.add_edge(i,n_students+j)\n",
    "\n",
    "\n",
    "# # COOL DRAGON-LIKE VISUALIZATION\n",
    "# # pp.visualisation.plot(n, width =800,height = 800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
