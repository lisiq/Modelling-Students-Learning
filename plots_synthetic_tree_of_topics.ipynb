{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from manage_experiments import *\n",
    "\n",
    "from utils import generate_multidimensional_data_object_synthetic_geometric\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills assignment completed!\n",
      "Balaceness 0.3199353084753835\n",
      "network density 0.49993055555555554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  student={\n",
       "    node_id=[720],\n",
       "    x=[720, 720],\n",
       "  },\n",
       "  item={\n",
       "    node_id=[280],\n",
       "    x=[280, 280],\n",
       "  },\n",
       "  (student, responds, item)={\n",
       "    edge_index=[2, 100786],\n",
       "    edge_attr=[100786, 1],\n",
       "    y=[100786],\n",
       "  },\n",
       "  (item, rev_responds, student)={ edge_index=[2, 100786] }\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathpy as pp\n",
    "from collections import defaultdict \n",
    "def generate_tree_of_topics(max_depth = 3,branching_factor = 2,root = \"*\"):\n",
    "    nodes_at_layer = defaultdict(list)\n",
    "    # initializing the tree with root node and first layer\n",
    "    start_nodes = [str(i) for i in range(branching_factor)]\n",
    "    nodes_at_layer[0].append(root)\n",
    "    nodes_at_layer[1].extend(start_nodes)\n",
    "    for depth in range(2,max_depth+1):\n",
    "        for node in nodes_at_layer[depth-1]:\n",
    "            for base_node in start_nodes:\n",
    "                nodes_at_layer[depth].append(node+base_node)\n",
    "\n",
    "    # creating network object\n",
    "    # children nodes connect to parent accoridn to condition below\n",
    "    n = pp.Network(directed = True)\n",
    "    for depth in range(max(nodes_at_layer)):\n",
    "        if depth == 0:\n",
    "            for node in nodes_at_layer[1]:\n",
    "                n.add_edge(root,node)\n",
    "        else:\n",
    "            for node_parent in nodes_at_layer[depth]:\n",
    "                for node_child in nodes_at_layer[depth+1]:\n",
    "                    # condition\n",
    "                    if node_child[:-1]==node_parent:\n",
    "                        n.add_edge(node_parent,node_child)\n",
    "    return n\n",
    "\n",
    "\n",
    "def assign_skills(n, study_hours, tau = 0, root=\"*\"):\n",
    "    # gives skills to students\n",
    "    # we start by taking one random path from root to leaf for each student\n",
    "    # EXTENSIONS goes in the direction of using personalized pagerank from the root node and would include: \n",
    "    #   study hours (similar to ability, encodes number of transitions taken by the students rw)\n",
    "    #   restart probability tau at any node\n",
    "    #   once an end node is reached, student restards from prev visited node (or root) with prob 1\n",
    "    #\n",
    "    # for now just enough to reach one leaf\n",
    "    T = n.transition_matrix().todense().T\n",
    "    root_index = n.node_to_name_map()[root]\n",
    "    ix_to_node = {v:k for k,v in n.node_to_name_map().items()}\n",
    "    last_node_ix = root_index\n",
    "    # saving what topics have been traversed\n",
    "    traversed_topics = set()\n",
    "    # random walk\n",
    "    studied_hours = 0\n",
    "    while studied_hours < study_hours:\n",
    "        # random restart (from root) probability\n",
    "        if np.random.rand() < tau:\n",
    "            last_node_ix = root_index\n",
    "        # reached a leaf node, save the topic and restart from root\n",
    "        if np.isclose(sum(T[last_node_ix,:].tolist()[0]),0.0):\n",
    "            last_node_ix = root_index\n",
    "        next_nodes_probability = T[last_node_ix,:].tolist()[0]\n",
    "        # If not in leaf node, and no random restart from root, randomly follow an edge to chidren node\n",
    "        last_node_ix = np.random.choice(range(n.ncount()),p=next_nodes_probability)\n",
    "        current_topic = ix_to_node[last_node_ix]\n",
    "        # if topics has not been traversed, add one study hour\n",
    "        if current_topic not in traversed_topics:\n",
    "            traversed_topics.add(current_topic)\n",
    "            studied_hours+=1\n",
    "    return traversed_topics #ix_to_node[last_node_ix]\n",
    "    # NB: could potentially keep only the nodes further down the tree\n",
    "\n",
    "\n",
    "def generate_observations_tree_of_topics(n, n_students : int, n_items : int,tree_depth :int, branching_factor:int, p_ER = .3,skill_mismatch_penalty = 100):\n",
    "    \"\"\"\n",
    "    Assigns topics to items and skills to items based on the tree of topics. \n",
    "    Then, generates observations. \n",
    "    \"\"\"\n",
    "    # ASSIGN TOPICS TO ITEMS AND SKILLS TO STUDENTS\n",
    "    #\n",
    "    # TASKS:\n",
    "    topics = list(set(n.nodes).difference({\"*\"}))\n",
    "    # NB: randomly sampling nodes will disproportionately give leaves, i.e., difficult items.\n",
    "    # equal probability to each layer, then equal for each node in the layer\n",
    "    # layer_probability = np.ones(tree_depth)/tree_depth \n",
    "    # probability proportional to number of nodes in each layer (i.e., similar to randomly sampling a node)\n",
    "    # layer_probability = [branching_factor**l/sum([branching_factor**i for i in range(1,tree_depth+1)]) for l  in range(1,tree_depth+1)]\n",
    "    # probability of sampling from one layer is proportional to the layer number\n",
    "    layer_probability = np.arange(1,tree_depth+1)/ np.arange(1,tree_depth+1).sum()\n",
    "    item_topics = np.random.choice(a=topics,size=n_items, p = [layer_probability[len(t)-1]/(branching_factor**(len(t))) for t in topics], replace=True)\n",
    "    dict_items_topics = dict(zip(range(n_items),item_topics))\n",
    "    #\n",
    "    # STUDENTS\n",
    "    dict_students_skills = {stud_id:set() for stud_id in range(n_students)}\n",
    "    for stud_id in dict_students_skills:\n",
    "        # TODO: deal with these parameters\n",
    "        study_hours = np.random.choice([round(i) for i in np.linspace(tree_depth,min(sum([branching_factor**i for i in range(1,tree_depth+1)]),tree_depth*10),10)])\n",
    "        dict_students_skills[stud_id].update(assign_skills(n,study_hours))\n",
    "    print(\"Skills assignment completed!\")\n",
    "    # GENERATE OBSERVATIONS\n",
    "    # students interact randomly with items (erdos renyi). Student pass items if they have traversed the topic/tree-node in their learning path\n",
    "    edges = []\n",
    "    labels = []\n",
    "    # TODO: could save p of passing and then adjust adjust a threshold to avoid or diminis the imbalance in the classification\n",
    "    for stud_id in dict_students_skills:\n",
    "        for item_id in dict_items_topics:\n",
    "            if np.random.rand()<p_ER:\n",
    "                edges.append((stud_id,item_id))\n",
    "                # deciding if item is passed\n",
    "                # could be made more nuanced checking how much of the path to the item_topic has been traversed by the student\n",
    "                item_topic = dict_items_topics[item_id] \n",
    "                student_skills = dict_students_skills[stud_id]\n",
    "                # number of steps (in the tree) needed to go from skills available to the student to the ones necessary to pass the task\n",
    "                steps_to_topic = 0\n",
    "                p_of_passing = 1\n",
    "                while ((item_topic[:-steps_to_topic] if steps_to_topic!=0 else item_topic) not in student_skills) and (steps_to_topic <= tree_depth):\n",
    "                    steps_to_topic+=1\n",
    "                p_of_passing = p_of_passing/((branching_factor+skill_mismatch_penalty)**(steps_to_topic))\n",
    "                if p_of_passing >= np.random.rand():\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "    print(\"Balaceness\",sum(labels)/len(labels))\n",
    "    return edges, labels\n",
    "\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "def create_data_object_synthetic_tree_topics(n_students,n_tasks,edge_indices, y):\n",
    "    data  = HeteroData()\n",
    "\n",
    "    # Save node indices\n",
    "    data['student'].node_id = torch.arange(n_students)\n",
    "    data['item'].node_id = torch.arange(n_tasks)\n",
    "\n",
    "\n",
    "    # Add the node features\n",
    "    data['student'].x= torch.eye(n_students)\n",
    "    data['item'].x = torch.eye(n_tasks)\n",
    "\n",
    "    # Add the edge indices\n",
    "    data['student', 'responds', 'item'].edge_index = torch.from_numpy(np.array(edge_indices).T).to(torch.long)\n",
    "\n",
    "    #add the edge attrs\n",
    "    data['student', 'responds', 'item'].edge_attr = torch.tensor([1]*len(y)).to(torch.float).reshape(-1,1)\n",
    "\n",
    "    # Add the edge label\n",
    "    data['student', 'responds', 'item'].y = torch.from_numpy(np.array(y)).to(torch.long)\n",
    "\n",
    "    # We use T.ToUndirected() to add the reverse edges from subject to students \n",
    "    # in order to let GNN pass messages in both ways\n",
    "    data = T.ToUndirected()(data)\n",
    "    del data['item', 'rev_responds', 'student'].edge_attr  # Remove 'reverse' label.\n",
    "    del data['item', 'rev_responds', 'student'].y  # Remove 'reverse' label.\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_data_object_synthetic_tree_topics(n_students, n_tasks, tree_depth,branching_factor=2, p_ER =.3, skill_mismatch_penalty = 100):\n",
    "    n = generate_tree_of_topics(tree_depth, branching_factor)\n",
    "    edges, labels = generate_observations_tree_of_topics(n, n_students, n_tasks, tree_depth,branching_factor,p_ER,skill_mismatch_penalty)\n",
    "    data = create_data_object_synthetic_tree_topics(n_students,n_tasks,edges, labels)\n",
    "    print(\"network density\",data.num_edges/(2*n_students*n_tasks))\n",
    "    return data\n",
    "\n",
    "\n",
    "data_object = generate_data_object_synthetic_tree_topics(n_students = 720, n_tasks = 280, tree_depth = 3,branching_factor=4, p_ER =.5)\n",
    "data_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def line_plot_with_std(list_scores, parameters, fname):\n",
    "\n",
    "    # Convert the list of lists to a Pandas DataFrame\n",
    "    df = pd.DataFrame(list_scores)\n",
    "\n",
    "    # Calculate the mean and standard deviation of each row\n",
    "    mean = df.mean(axis=1)\n",
    "    std = df.std(axis=1)\n",
    "\n",
    "    # Create a figure and an axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the mean line\n",
    "    ax.plot(mean, color='blue', label='Mean')\n",
    "\n",
    "    # Plot the shaded region around the mean line\n",
    "    ax.fill_between(df.index, mean-std, mean+std, color='lightblue', alpha=0.5, label='Standard deviation')\n",
    "\n",
    "    # Add some labels and a legend\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Value')\n",
    "    # ax.legend()\n",
    "\n",
    "    # change x-ticks to start from \n",
    "    n_tasks_per_student_list = list(range(\n",
    "        parameters['min_n_tasks_per_student'],\n",
    "        parameters['max_n_tasks_per_student'],\n",
    "        parameters['step_n_tasks_per_student']))\n",
    "    ax.set_xticks(range(len(n_tasks_per_student_list)), n_tasks_per_student_list, rotation=45)\n",
    "\n",
    "    # Show and save the plot\n",
    "    plt.grid()\n",
    "    plt.xlabel('Number of tasks per student')\n",
    "    plt.ylabel('Balanced Accuracy')\n",
    "    fnameplot = fname.replace('results', 'pdf')\n",
    "    plt.savefig(fnameplot)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_confusion_matrix(json_content, fname):\n",
    "    fig, ax = plt.subplots(ncols = 1, figsize=(8,6))\n",
    "    \n",
    "    plt.imshow(json_content[\"Confusion_2_test\"], interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "\n",
    "    classes = [0, 1]\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    part1, part2 = fname.split('.')\n",
    "    part1 = part1 + '_consfusion_matrix'\n",
    "    fname = '.'.join([part1, part2])\n",
    "    fnameplot = fname.replace('results', 'pdf')\n",
    "    plt.savefig(fnameplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.utils import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "def create_filename_results(parameters,fold):\n",
    "    file_name = ' '.join([f\"{str(key)}-{str(value)}\" for key, value in parameters.items() if key not in ['max_n_tasks_per_student','min_n_tasks_per_student']])\n",
    "    file_name += ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(4))\n",
    "    file_name.replace(\".\",\"\")\n",
    "    file_name+=\".results\"\n",
    "    return os.path.join(fold, file_name)\n",
    "\n",
    "\n",
    "#n_students_per_task_list = [2**i for i in n_students_per_task_list]\n",
    "\n",
    "import json\n",
    "def synthetic_density_effect_runs(parameters,parameters_density, folder = 'synth_density_effect'):\n",
    "    parameters = {**parameters, **parameters_density}\n",
    "    scores = []\n",
    "    list_scores = []\n",
    "    for _ in range(parameters[\"n_runs\"]):\n",
    "        with io.capture_output() as captured:\n",
    "            data = generate_data_object_synthetic_tree_topics(\n",
    "                n_students = parameters['n_students'], \n",
    "                n_tasks = parameters['n_tasks'],\n",
    "                tree_depth=parameters[\"tree_depth\"],\n",
    "                branching_factor=parameters[\"branching_factor\"],\n",
    "                p_ER=parameters[\"p_ER\"],\n",
    "                skill_mismatch_penalty=parameters[\"skill_mismatch_penalty\"])\n",
    "\n",
    "            cv_out = perform_cross_validation(data, parameters, save_embeddings=True)\n",
    "\n",
    "            score = 0\n",
    "            inner_list_scores = []\n",
    "            for fold_n in range(parameters[\"n_splits\"]):\n",
    "                score += cv_out[f\"Balanced Accuracy_{fold_n}_test\"]\n",
    "                inner_list_scores.extend([cv_out[f\"Balanced Accuracy_{fold_n}_test\"]])\n",
    "            score = score / parameters[\"n_splits\"]\n",
    "            scores.append(score)\n",
    "            list_scores.append(inner_list_scores)\n",
    "\n",
    "    density = data.num_edges/(2*parameters[\"n_students\"]*parameters[\"n_tasks\"]) \n",
    "    res_dict = dict(zip(list(range(parameters[\"n_runs\"])),scores))\n",
    "    output_dict = {\n",
    "        **parameters,\n",
    "        \"density\":density,\n",
    "        \"res_dict\":res_dict,\n",
    "        \"list_scores\":dict(zip(list(range(parameters[\"n_runs\"])),list_scores))\n",
    "    }\n",
    "    fname = create_filename_results(parameters_density, folder)\n",
    "    with open(fname,'w') as f:\n",
    "        json.dump(output_dict, f, skipkeys=True)\n",
    "    \n",
    "    # save results\n",
    "    # print(density)\n",
    "    print([np.mean(s) for s in list_scores])\n",
    "    # line_plot_with_std(list_scores=list_scores, parameters= parameters, fname=fname)\n",
    "\n",
    "    # # save embeddings\n",
    "    # visualise_embeddings(output_dict, fname=fname)\n",
    "\n",
    "    # visualise_embeddings_concatenated(output_dict, data['student', 'responds', 'item'].edge_index, data['student', 'item'].y, fname = fname) # plot with indices of train-test split\n",
    "    \n",
    "    # # plot with indices of train-test split\n",
    "    \n",
    "\n",
    "\n",
    "    # visualise_confusion_matrix(cv_out, fname=fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"hidden_dims\": None,\n",
    "    'model_type': None,\n",
    "    \"df_name\": \"synthetic.salamoia\",\n",
    "    \"method\": \"EdgeClassifier\",\n",
    "    \"epochs\": 1000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"weight_decay\": 0,\n",
    "    \"dropout\": 0.0,\n",
    "    \"early_stopping\": 250,\n",
    "    \"n_splits\": 10,\n",
    "    \"device\": \"cuda\",\n",
    "    \"done\": False,\n",
    "    \"batch_size\":128,\n",
    "    #\n",
    "    }\n",
    "\n",
    "parameters_density = {\n",
    "    'n_students' :720, # 72% of nodes were students\n",
    "    'n_tasks':280,\n",
    "    'n_runs':5,\n",
    "    'tree_depth':None,\n",
    "    'branching_factor':None,\n",
    "    'p_ER':None, \n",
    "    'skill_mismatch_penalty': None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964\n",
    "# def seed_everything(seed: int):\n",
    "#     import random, os\n",
    "#     import numpy as np\n",
    "#     import torch\n",
    "    \n",
    "#     random.seed(seed)\n",
    "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tree depth  5\n",
      "branching_factor 2\n",
      "p_ER 0.1\n",
      "skill_mismatch_penalty 0\n",
      "IRT\n",
      "[0.7468951540114903, 0.7263914403973422, 0.7307473642183006, 0.7293453570808309, 0.7181132637942257]\n",
      "GNN\n",
      "[0.7417417258154988, 0.7405556110155125, 0.7492455471415428, 0.7495842533864181, 0.7427250234012361]\n",
      "skill_mismatch_penalty 1000\n",
      "IRT\n",
      "[0.7657938028817137, 0.7531589419445679, 0.7574995874333181, 0.7549052749056301, 0.7596725082615999]\n",
      "GNN\n",
      "[0.763228284784501, 0.7687549084718219, 0.7717763013885597, 0.7693762795584883, 0.7653712199058166]\n",
      "p_ER 0.3\n",
      "skill_mismatch_penalty 0\n",
      "IRT\n",
      "[0.7919142523912612, 0.7851387971964006, 0.7817601369271667, 0.7833377202049859, 0.7892780257195504]\n",
      "GNN\n",
      "[0.7607483071568248, 0.7560544204592097, 0.7599159435421013, 0.7593818353212162, 0.753418163397731]\n",
      "skill_mismatch_penalty 1000\n",
      "IRT\n",
      "[0.8387259979300836, 0.8443168749144216, 0.8388777251070824, 0.8384648992458177, 0.8412050719838129]\n",
      "GNN\n",
      "[0.7840725759122784, 0.780902471054448, 0.7734504853702351, 0.7866567157186765, 0.7844531132951909]\n",
      "p_ER 0.5\n",
      "skill_mismatch_penalty 0\n",
      "IRT\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model_type\n\u001b[0;32m     29\u001b[0m parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_dims\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hidden_dims\n\u001b[1;32m---> 30\u001b[0m \u001b[43msynthetic_density_effect_runs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mparameters_density\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters_density\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 30\u001b[0m, in \u001b[0;36msynthetic_density_effect_runs\u001b[1;34m(parameters, parameters_density, folder)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mcapture_output() \u001b[38;5;28;01mas\u001b[39;00m captured:\n\u001b[0;32m     22\u001b[0m     data \u001b[38;5;241m=\u001b[39m generate_data_object_synthetic_tree_topics(\n\u001b[0;32m     23\u001b[0m         n_students \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_students\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m     24\u001b[0m         n_tasks \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_tasks\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m         p_ER\u001b[38;5;241m=\u001b[39mparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp_ER\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     28\u001b[0m         skill_mismatch_penalty\u001b[38;5;241m=\u001b[39mparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskill_mismatch_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 30\u001b[0m     cv_out \u001b[38;5;241m=\u001b[39m \u001b[43mperform_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     33\u001b[0m     inner_list_scores \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\liq02qc\\Documents\\GitHub\\Modelling-Students-Learning\\manage_experiments.py:205\u001b[0m, in \u001b[0;36mperform_cross_validation\u001b[1;34m(data, parameters, save_embeddings, save_subgraph, model, final_fit)\u001b[0m\n\u001b[0;32m    197\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    198\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train_loop(\n\u001b[0;32m    199\u001b[0m         model,\n\u001b[0;32m    200\u001b[0m         batch, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m#class_weights\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         )\n\u001b[1;32m--> 205\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    207\u001b[0m val_b \u001b[38;5;241m=\u001b[39m test_loop(model, val_subgraph_data, fold, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    208\u001b[0m test_b \u001b[38;5;241m=\u001b[39m test_loop(model, test_subgraph_data, fold, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NB: having only the leaves might make it easier for IRT since it just creates separated bins (i.e., the leaves) and easy tasks (the ones sampled close to the root node)\n",
    "#  Instead, with a GNN we want to leverage the ...ability to encode the underlying tree structure (HOW might the GNN do it, specifically?)\n",
    "depths = [5,6,7,8]\n",
    "branching_factors = [2,3,4]\n",
    "p_ERs = [.1,.3,.5]\n",
    "smps = [0,1000]\n",
    "\n",
    "for tree_depth in depths:\n",
    "    parameters_density['tree_depth'] = tree_depth\n",
    "    print()\n",
    "    print('Tree depth ', tree_depth)\n",
    "    for branching_factor in branching_factors:\n",
    "        parameters_density['branching_factor'] = branching_factor\n",
    "        print('branching_factor', branching_factor)\n",
    "        for p_ER in p_ERs:\n",
    "            parameters_density['p_ER'] = p_ER\n",
    "            print('p_ER', p_ER)\n",
    "            for smp in smps:\n",
    "                parameters_density['skill_mismatch_penalty'] = smp\n",
    "                print('skill_mismatch_penalty', smp)\n",
    "                for model_type,hidden_dims in [\n",
    "                    (\"IRT\",3),\n",
    "                    (\"GNN\", [\n",
    "                        16,\n",
    "                        16,\n",
    "                        8])]:\n",
    "                    print(model_type)\n",
    "                    parameters[\"model_type\"] = model_type\n",
    "                    parameters[\"hidden_dims\"] = hidden_dims\n",
    "                    synthetic_density_effect_runs(parameters=parameters,\n",
    "                                                parameters_density=parameters_density,\n",
    "                                                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
