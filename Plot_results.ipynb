{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET = 'matrix'\n",
    "OUTNAME = 'SAGE'\n",
    "NPERMS = 10\n",
    "DIFFICULTY_BINS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import to_hetero\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from cluster_utils import evaluate_items\n",
    "from vis_utils import (visualize_students, visualize_edges_age, visualize_edges, visualize_items, myresults, FIGSIZE,\n",
    "CLUSTER_LABELS,  AGE_THR, DPI, CPU_Unpickler, plot_clustering, FONTSCALE)\n",
    "\n",
    "sns.set_theme(context='talk', style='white', font_scale=FONTSCALE)\n",
    "\n",
    "FILENAME = f'{OUTNAME}_{DATASET}'\n",
    "EQUAL_AXES=False\n",
    "MINSAMPLES = 10 #if DATASET in ['matrix', 'topic'] else 10\n",
    "\n",
    "RELOAD = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './results/SAGE_matrix.pth.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./results/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mFILENAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth.tar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/GNN/lib/python3.8/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/GNN/lib/python3.8/site-packages/torch/serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/GNN/lib/python3.8/site-packages/torch/serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './results/SAGE_matrix.pth.tar'"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = 'cpu'\n",
    "model = torch.load(f'./results/{FILENAME}.pth.tar', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./results/{FILENAME}_aux_data.pkl', 'rb') as handle:\n",
    "    aux_data = CPU_Unpickler(handle).load()\n",
    "    \n",
    "(df, df_student, df_item, df_edge, \n",
    "            train_losses, #test_losses, test_aucs,\n",
    "            train_edge_indices, val_edge_indices, test_edge_indices, \n",
    "            data, train_data, val_data, test_data,\n",
    "            unique_scales, unique_domains, unique_matdiff, \n",
    "            DATA_FILE) = aux_data\n",
    "print(df_item.shape)\n",
    "\n",
    "try:\n",
    "    # if IRT data exists, take item parameters\n",
    "    with open(f'./results/IRT_{DATASET}_aux_data_IRT1.pkl', 'rb') as handle:\n",
    "        aux_data = CPU_Unpickler(handle).load()\n",
    "\n",
    "    (_, _, df_item, *_) = aux_data\n",
    "except:\n",
    "    pass\n",
    "print(df_item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RELOAD:\n",
    "    df = load_data_heterogeneous(DATA_FILE)\n",
    "    data, df_student, df_item, df_edge = create_data_object_heterogeneous(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "age_ranges = df.groupby('studentId').age.apply(lambda x: x.max() - x.min())\n",
    "fig = plt.figure()\n",
    "age_ranges.plot.hist(bins=100)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "df.age.plot.hist(bins=100)\n",
    "age_ranges = df.groupby('studentId').age.apply(lambda x: x.max() - x.min())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different items per student\n",
    "df.groupby(['studentId']).code.nunique().hist(bins=100)\n",
    "df.groupby(['studentId']).code.nunique().median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['assessmentId']).code.nunique().hist(bins=100)\n",
    "df.groupby(['assessmentId']).code.nunique().median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_ranges.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.arange(len(train_losses))\n",
    "fig = plt.figure()\n",
    "plt.plot(train_indices, train_losses, c='blue')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(f'./vis/{FILENAME}_losses.png', dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove"
    ]
   },
   "source": [
    "df_item['IRT_difficulty_binned'] = pd.cut(df_item['IRT1_difficulty'], DIFFICULTY_BINS).astype(str)\n",
    "df_item['scalexdifficulty'] = df_item.apply(lambda x: x.scale + ' ' + \n",
    "                                            x.IRT_difficulty_binned if x.IRT_difficulty_binned != 'nan' else pd.NA, axis=1)\n",
    "\n",
    "df_item.groupby('scalexdifficulty').matrix.count().sort_values(ascending=False).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(df_item['IRT1_difficulty'], df_item['IRT1_discrimination_transf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove"
    ]
   },
   "source": [
    "plot_clustering('scalexdifficulty', 'matrix', model, train_data, df_item, device, FILENAME, minsamples=MINSAMPLES, \n",
    "                nperms=NPERMS)\n",
    "plot_clustering('scalexdifficulty', 'topic', model, train_data, df_item, device, FILENAME, minsamples=MINSAMPLES, \n",
    "                nperms=NPERMS)\n",
    "\n",
    "\n",
    "if DATASET in ['matrix', 'topic']:\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myresults.output_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minlevels = 8\n",
    "MINSAMPLES = 20\n",
    "from cluster_utils import compute_clustering_indices\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import seaborn as sns\n",
    "\n",
    "dimred = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3)\n",
    "#dimred = PCA(whiten=False)\n",
    "\n",
    "df_item['IRT_difficulty_binned'] = pd.cut(df_item['IRT1_difficulty'], DIFFICULTY_BINS).astype(str)\n",
    "df_item['scalexdifficulty'] = df_item.apply(lambda x: x.scale + ' ' + \n",
    "                                         \n",
    "                                            x.IRT_difficulty_binned if x.IRT_difficulty_binned != 'nan' else pd.NA, axis=1)\n",
    "df_item.groupby('scalexdifficulty').matrix.count().sort_values(ascending=False).plot()\n",
    "\n",
    "\n",
    "grouping_variable, target_variable = 'scalexdifficulty', 'matrix'\n",
    "scores_dict = {'CH': [], 'DB':[], 'N_LEVELS':[], 'N_SAMPLES':[]}\n",
    "scores = compute_clustering_indices(model, train_data, df_item.copy(), device, grouping_variable, \n",
    "                                        target_variable, shuffle=False, seed=0, minsamples=MINSAMPLES)\n",
    "[ scores_dict[key].append(scores[key]) for key in scores_dict]\n",
    "\n",
    "scores_df = pd.DataFrame(scores_dict['DB'])        \n",
    "scores_df = pd.melt(scores_df, value_name='index', var_name=grouping_variable).sort_values('index', ascending=False)\n",
    "\n",
    "scores_df['N_LEVELS'] = scores_df[grouping_variable].apply(lambda x: scores_dict['N_LEVELS'][0][x])\n",
    "scores_df['N_SAMPLES'] = scores_df[grouping_variable].apply(lambda x: scores_dict['N_SAMPLES'][0][x])\n",
    "\n",
    "scores_df = scores_df.loc[scores_df['N_LEVELS'] >= minlevels]\n",
    "\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "    \n",
    "def plot_clusters_slice(grouping_variable, target_variable, df_item, model, data, scores_df, k=0):\n",
    "    df_item_ = df_item.reset_index().drop(columns='index')\n",
    "\n",
    "    df_item_sub = df_item_.loc[df_item_[grouping_variable] == scores_df.iloc[k][grouping_variable]]\n",
    "    \n",
    "    # look only at those with a minimum of samples\n",
    "    tab = df_item_sub[target_variable].value_counts()\n",
    "    print(tab)\n",
    "    #ind = tab[tab >= minsamples].index\n",
    "    ind = tab[:4].index\n",
    "    df_item_sub = df_item_sub.loc[df_item_sub[target_variable].isin(ind).values]\n",
    "    if target_variable == 'matrix':\n",
    "        df_item_sub['labels'] = df_item_sub[target_variable].apply(lambda x: '.'.join(x.split('.')[2:]))\n",
    "    else:\n",
    "        df_item_sub['labels'] = df_item_sub[target_variable]\n",
    "    print(df_item_sub[target_variable].value_counts())\n",
    "    data = data.to(device)\n",
    "\n",
    "    z_dict = model.get_embeddings(data, encoded=True)\n",
    "    embedding = z_dict['item'].detach().cpu().numpy()\n",
    "    #dimred.fit(embedding)\n",
    "    #low_dim = dimred.transform(embedding)\n",
    "\n",
    "    low_dim = dimred.fit_transform(embedding)\n",
    "    low_dim = low_dim[df_item_sub.index, :]\n",
    "\n",
    "    X = df_item_sub #.loc[select, :]\n",
    "\n",
    "    #X['domain'] = X['domain'].apply(lambda x: DOMAIN_LABELS[x])\n",
    "\n",
    "    X['x'] = low_dim[:, 0]\n",
    "    X['y'] = low_dim[:, 1]\n",
    "     #   X['z'] = low_dim[:, 2]\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    axes = sns.scatterplot(data=X, x='x', y='y', hue='labels', s=100, alpha=0.7) \n",
    "    axes.legend(prop = { 'size': 16 })\n",
    "    plt.show()\n",
    "\n",
    "    clustering = AgglomerativeClustering(distance_threshold=0, n_clusters=None, linkage='complete')\n",
    "    clustering.fit(low_dim)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "    # plot the top three levels of the dendrogram\n",
    "\n",
    "    plot_dendrogram(clustering, truncate_mode=\"level\", p=10, labels=df_item_sub['labels'].values)\n",
    "    plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "    plt.show()\n",
    "    \n",
    "for k in range(4):\n",
    "    plot_clusters_slice(grouping_variable, target_variable, df_item, model, data, scores_df, k=k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# represent embeddings before and after the GNN encoder\n",
    "visualize_items(model, train_data, device, df_item.copy(), FILENAME, equal_axes=EQUAL_AXES, encoded=False)\n",
    "\n",
    "visualize_items(model, train_data, device, df_item.copy(), FILENAME, equal_axes=EQUAL_AXES, encoded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {\n",
    "    'matrix':{'CH': [], 'SH': [], 'DB':[]},\n",
    "    'topic':{'CH': [], 'SH': [], 'DB':[]}\n",
    "    }\n",
    "distances_dict = {'within': [], 'between': []}\n",
    "mean_distance_list = []\n",
    "#NPERMS = 10\n",
    "# perm == 0 is unshuffled\n",
    "for perm in range(NPERMS):\n",
    "    scores_matrix, scores_topic, within_domain, between_domain, mean_distances, unique_scales, within_between_scales = evaluate_items(model, train_data, df_item, device, \n",
    "                                                                           shuffle=perm>0, \n",
    "                                                                           seed=0, \n",
    "                                                                           minsamples=MINSAMPLES)\n",
    "    \n",
    "    [ scores_dict['matrix'][key].append(scores_matrix[key]) for key in scores_dict['matrix']]\n",
    "    [ scores_dict['topic'][key].append(scores_matrix[key]) for key in scores_dict['topic']]\n",
    "    distances_dict['within'].append(within_domain)\n",
    "    distances_dict['between'].append(between_domain)\n",
    "    mean_distance_list.append(within_between_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_df = pd.DataFrame(np.concatenate(mean_distance_list), columns=unique_scales) \n",
    "distances_df['perm'] = distances_df.index // len(unique_scales)\n",
    "distances_df['random'] = 'Observed\\n data'\n",
    "distances_df.loc[ distances_df['perm'] > 0, 'random'] = 'Shuffled\\n data'\n",
    "distances_df['within_between'] = 'between'\n",
    "distances_df.loc[ distances_df.index % len(unique_scales) == 0, 'within_between'] = 'within'\n",
    "\n",
    "distances_df = distances_df.groupby(['perm','within_between','random'])[unique_scales].mean().reset_index()\n",
    "distances_df = pd.melt(distances_df, id_vars=['perm','within_between','random'], value_name='distance', var_name='scale')\n",
    "\n",
    "# between - within distance should be positive\n",
    "distances_df['diff'] = distances_df['distance'] \n",
    "aux = distances_df.query('`within_between` == \"between\"').copy().reset_index()\n",
    "within = distances_df.query('`within_between` == \"within\"').reset_index()['distance']\n",
    "aux['diff'] = (aux['diff'] - within)/within*100\n",
    "\n",
    "distances_df = aux.drop(columns='within_between').sort_values(by=['scale', 'random'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_df.query('perm == 0' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = sns.barplot(data=distances_df, x='scale', y='diff', hue='random', errorbar='sd')\n",
    "axes.legend_.remove()\n",
    "axes.tick_params(axis='x', rotation=90)\n",
    "axes.set(xlabel='Competence Domain', ylabel='Relative Distance \\n Difference (%)')\n",
    "fig.tight_layout()\n",
    "plt.savefig(f'./vis/{FILENAME}_scale_distance_bw.png', dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_df = pd.DataFrame(distances_dict)\n",
    "distances_df['perm'] = distances_df.index\n",
    "#distances_df = pd.melt(distances_df, id_vars='perm', value_name='index', var_name='group')\n",
    "distances_df['random'] = 'Observed \\n data'\n",
    "distances_df.loc[ distances_df['perm'] > 0, 'random'] = 'Shuffled \\ndata'\n",
    "distances_df['diff'] = (distances_df['between'] - distances_df['within'] )/distances_df['within']*100\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = sns.barplot(data=distances_df, x='random', y='diff', errorbar='sd')\n",
    "ax.set(xlabel='', ylabel='Relative Distance \\n Difference (%)')\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(f'./vis/{FILENAME}_domain_distance_bw.png', dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_df.query('perm == 0' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "#from matplotlib.category import UnitData\n",
    "\n",
    "for variable in ['matrix']: # , 'topic'\n",
    "    for i, index in enumerate(['CH', 'DB']): #scores_dict\n",
    "        fig = plt.figure(figsize=(FIGSIZE))\n",
    "\n",
    "        scores_df = pd.DataFrame(scores_dict[variable][index])\n",
    "        scores_df['perm'] = scores_df.index\n",
    "        scores_df = pd.melt(scores_df, id_vars='perm', value_name='index', var_name='scale')\n",
    "        scores_df['random'] = 'Observed data'\n",
    "        scores_df.loc[ scores_df['perm'] > 0, 'random'] = 'Shuffled data'\n",
    "        #plt.figure()\n",
    "        scores_df['scale'] = pd.Categorical(scores_df['scale'], categories=np.sort(scores_df.scale.unique()))\n",
    "        axes = sns.barplot(data=scores_df, x='scale', y='index', hue='random', errorbar='sd')\n",
    "        axes.legend_.remove()\n",
    "        axes.set_xlabel('Competence Domain')\n",
    "        axes.set_ylabel('Cluster Validity Index')\n",
    "        axes.set_title(CLUSTER_LABELS[index])\n",
    "        axes.tick_params(axis='x', rotation=90)\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(f'./vis/{FILENAME}_{variable}_bw_random_{index}.png', dpi=DPI)\n",
    "        plt.close()\n",
    "        print(scores_df.query('perm == 0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item['IRT_difficulty_binned'] = pd.cut(df_item['IRT1_difficulty'], DIFFICULTY_BINS).astype(str)\n",
    "df_item['scalexdifficulty'] = df_item.apply(lambda x: x.scale + ' ' + \n",
    "                                         \n",
    "                                            x.IRT_difficulty_binned if x.IRT_difficulty_binned != 'nan' else pd.NA, axis=1)\n",
    "df_item.groupby('scalexdifficulty').matrix.count().sort_values(ascending=False).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_clustering('scalexdifficulty', 'topic', model, train_data, df_item, device, FILENAME, minsamples=MINSAMPLES, \n",
    "#                nperms=NPERMS)\n",
    "plot_clustering('scalexdifficulty', 'matrix', model, train_data, df_item, device, FILENAME, minsamples=MINSAMPLES, \n",
    "                nperms=NPERMS)\n",
    "#if DATASET in ['matrix', 'topic']:\n",
    "#    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# represent embeddings before and after the GNN encoder\n",
    "visualize_items(model, train_data, device, df_item.copy(), FILENAME, equal_axes=EQUAL_AXES, encoded=False)\n",
    "\n",
    "visualize_items(model, train_data, device, df_item.copy(), FILENAME, equal_axes=EQUAL_AXES, encoded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_students(model, train_data, device, df_student, FILENAME, equal_axes=EQUAL_AXES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_edges(model, train_data, train_edge_indices, device, df, FILENAME, \n",
    "#                equal_axes=EQUAL_AXES, with_lines=True, aggregate=True, AGE_THR=AGE_THR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_edges(model, train_data, train_edge_indices, device, df, FILENAME, \n",
    "#                equal_axes=EQUAL_AXES, with_lines=False, aggregate=False, AGE_THR=AGE_THR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stophere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cluster_utils import compute_clustering_indices\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import seaborn as sns\n",
    "\n",
    "grouping_variable, target_variable = 'scalexdifficulty', 'topic'\n",
    "scores_dict = {'CH': [], 'DB':[], 'N_LEVELS':[], 'N_SAMPLES':[]}\n",
    "scores = compute_clustering_indices(model, train_data, df_item.copy(), device, grouping_variable, \n",
    "                                        target_variable, shuffle=False, seed=0, minsamples=MINSAMPLES)\n",
    "[ scores_dict[key].append(scores[key]) for key in scores_dict]\n",
    "\n",
    "scores_df = pd.DataFrame(scores_dict['DB'])        \n",
    "scores_df = pd.melt(scores_df, value_name='index', var_name=grouping_variable).sort_values('index', ascending=False)\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "    \n",
    "def plot_clusters_slice(grouping_variable, target_variable, df_item, model, data, k=0):\n",
    "    df_item_ = df_item.reset_index().drop(columns='index')\n",
    "\n",
    "    df_item_sub = df_item_.loc[df_item_[grouping_variable] == scores_df.iloc[k][grouping_variable]]\n",
    "    df_item_sub\n",
    "\n",
    "    dimred = PCA(whiten=False)\n",
    "    z_dict = model.get_embeddings(data, encoded=True)\n",
    "    embedding = z_dict['item'].detach().cpu().numpy()\n",
    "    dimred.fit(embedding)\n",
    "\n",
    "    low_dim = dimred.transform(embedding)\n",
    "    low_dim = low_dim[df_item_sub.index, :]\n",
    "\n",
    "    X = df_item_sub #.loc[select, :]\n",
    "\n",
    "    #X['domain'] = X['domain'].apply(lambda x: DOMAIN_LABELS[x])\n",
    "\n",
    "    X['x'] = low_dim[:, 0]\n",
    "    X['y'] = low_dim[:, 1]\n",
    "    X['z'] = low_dim[:, 2]\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    axes = sns.scatterplot(data=X, x='x', y='y', hue=target_variable, s=16, alpha=1) \n",
    "    axes.legend(prop = { 'size': 8 })\n",
    "    plt.show()\n",
    "\n",
    "    clustering = AgglomerativeClustering(distance_threshold=0, n_clusters=None, linkage='complete')\n",
    "    clustering.fit(low_dim)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "    # plot the top three levels of the dendrogram\n",
    "    plot_dendrogram(clustering, truncate_mode=\"level\", p=5, labels=df_item_sub[target_variable].values)\n",
    "    plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "    plt.show()\n",
    "    \n",
    "for k in range(3):\n",
    "    plot_clusters_slice(grouping_variable, target_variable, df_item, model, data, k=k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cluster_utils import compute_clustering_indices\n",
    "\n",
    "grouping_variable, target_variable = 'scalexdifficulty', 'matrix'\n",
    "scores_dict = {'CH': [], 'DB':[], 'N_LEVELS':[], 'N_SAMPLES':[]}\n",
    "scores = compute_clustering_indices(model, train_data, df_item.copy(), device, grouping_variable, \n",
    "                                        target_variable, shuffle=False, seed=0, minsamples=MINSAMPLES)\n",
    "[ scores_dict[key].append(scores[key]) for key in scores_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores_dict['DB'])        \n",
    "scores_df = pd.melt(scores_df, value_name='index', var_name=grouping_variable).sort_values('index', ascending=False)\n",
    "scores_df.iloc[0][grouping_variable]\n",
    "df_item_ = df_item.reset_index().drop(columns='index')\n",
    "df_item_sub = df_item_.loc[df_item_[grouping_variable] == scores_df.iloc[0][grouping_variable]]\n",
    "df_item_sub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "dimred = PCA(whiten=False)\n",
    "df_item\n",
    "z_dict = model.get_embeddings(data, encoded=True)\n",
    "embedding = z_dict['item'].detach().cpu().numpy()\n",
    "dimred.fit(embedding)\n",
    "\n",
    "low_dim = dimred.transform(embedding)\n",
    "low_dim = low_dim[df_item_sub.index, :]\n",
    "\n",
    "X = df_item_sub #.loc[select, :]\n",
    "\n",
    "#X['domain'] = X['domain'].apply(lambda x: DOMAIN_LABELS[x])\n",
    "\n",
    "X['x'] = low_dim[:, 0]\n",
    "X['y'] = low_dim[:, 1]\n",
    "X['z'] = low_dim[:, 2]\n",
    "\n",
    "import seaborn as sns\n",
    "axes = sns.scatterplot(data=X, x='x', y='y', hue=target_variable, s=16, alpha=1) \n",
    "axes.legend(prop = { 'size': 8 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "    \n",
    "clustering = AgglomerativeClustering(distance_threshold=0, n_clusters=None, linkage='complete')\n",
    "clustering.fit(low_dim)\n",
    "\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(clustering, truncate_mode=\"level\", p=5, labels=df_item_sub[target_variable].values)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_items(model, data, device, df_item, OUTNAME, dims=('x', 'y'), equal_axes=False, encoded=True):\n",
    "\n",
    "    if encoded:\n",
    "        suffix = ''    \n",
    "    else:\n",
    "        suffix = '_notenc'    \n",
    "        \n",
    "    data = data.to(device)\n",
    "    try:\n",
    "        pred, z_dict, z_edge = model(data)\n",
    "    except:\n",
    "        z_dict = model.get_embeddings(data, encoded=encoded)        \n",
    "    embedding = z_dict['item'].detach().cpu().numpy()\n",
    "\n",
    "    dimred.fit(embedding)\n",
    "    low_dim = dimred.transform(embedding)\n",
    "\n",
    "    X = df_item #.loc[select, :]\n",
    "    \n",
    "    X['domain'] = X['domain'].apply(lambda x: DOMAIN_LABELS[x])\n",
    "    \n",
    "    X['x'] = low_dim[:, 0]\n",
    "    X['y'] = low_dim[:, 1]\n",
    "    X['z'] = low_dim[:, 2]\n",
    "    figname = f'{OUTNAME}_items{suffix}'\n",
    "    save_plot(X, 'domain', 'Subject Domain', figname, x='x', y='y', plot_type='sct', equal_axes=equal_axes, with_legend=True)\n",
    "    save_plot(X, 'scale', 'Competence Domain', figname, x='x', y='y', plot_type='sct', equal_axes=equal_axes)\n",
    "    save_plot(X, 'IRT_difficulty', 'Difficulty', figname, x='x', y='y', plot_type='sct', equal_axes=equal_axes, palette='viridis')\n",
    "    save_plot(X, 'IRT1_difficulty', 'Difficulty', figname, x='x', y='y', plot_type='sct', equal_axes=equal_axes, palette='viridis')\n",
    "    save_plot(X, 'IRT1_discrimination', 'Discrimination', figname, x='x', y='y', plot_type='sct', equal_axes=equal_axes, palette='viridis')\n",
    "    save_plot(X, 'IRT1_discrimination_transf', 'Discrimination (transformed)', figname, x='x', y='y', plot_type='sct', equal_axes=equal_axes, palette='viridis')\n",
    "\n",
    "    save_plot(X, 'domain', 'Subject Domain', figname, x='z', y='y', plot_type='sct', equal_axes=equal_axes, with_legend=False)\n",
    "    save_plot(X, 'scale', 'Competence Domain', figname, x='z', y='y', plot_type='sct', equal_axes=equal_axes)\n",
    "    save_plot(X, 'IRT_difficulty', 'Difficulty', figname, x='z', y='y', plot_type='sct', equal_axes=equal_axes, palette='viridis')\n",
    "    save_plot(X, 'IRT1_difficulty', 'Difficulty', figname, x='z', y='y', plot_type='sct', equal_axes=equal_axes, palette='viridis')\n",
    "    save_plot(X, 'IRT1_discrimination', 'Discrimination', figname, x='z', y='y', plot_type='sct', equal_axes=equal_axes, palette='viridis')\n",
    "    save_plot(X, 'IRT1_discrimination_transf', 'Discrimination (transformed)', figname, x='z', y='y', plot_type='sct', equal_axes=equal_axes, palette='viridis')\n",
    "\n",
    "    figname = f'{OUTNAME}_dim_items{suffix}'\n",
    "    for i, mydim in enumerate(['x', 'y', 'z']):\n",
    "        save_plot(X, 'domain', 'Subject Domain', figname, x=mydim, plot_type='kde')\n",
    "        save_plot(X, 'scale', 'Competence Domain', figname, x=mydim, plot_type='kde')\n",
    "        save_plot(X, 'IRT_difficulty', 'Difficulty', figname, x=mydim, plot_type='reg')\n",
    "        save_plot(X, 'IRT1_difficulty', 'Difficulty', figname, x=mydim, plot_type='reg')\n",
    "        save_plot(X, 'IRT1_discrimination', 'Discrimination', figname, x=mydim, plot_type='reg')\n",
    "        save_plot(X, 'IRT1_discrimination_transf', 'Discrimination (transformed)', figname, x=mydim, plot_type='reg')\n",
    "        myresults.add_stats('item_difficulty_%s'%mydim, X['IRT_difficulty'], X[mydim])\n",
    "        myresults.add_stats('item_difficulty1_%s'%mydim, X['IRT1_difficulty'], X[mydim])\n",
    "        myresults.add_stats('item_discrimination1_%s'%mydim, X['IRT1_discrimination'], X[mydim])\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    PC_values = np.arange(dimred.n_components_) + 1\n",
    "    #plt.sca(axes[1, 1])\n",
    "    plt.plot(PC_values, dimred.explained_variance_ratio_*100, 'o-', linewidth=2, color='blue')\n",
    "    plt.plot(PC_values, np.cumsum(dimred.explained_variance_ratio_)*100, 'o-', linewidth=2, color='red')\n",
    "    fig.legend(labels=['Variance', 'Cumulative variance'], loc='center', fontsize=10)\n",
    "    \n",
    "    print(dimred.explained_variance_ratio_*100)\n",
    "    print(np.cumsum(dimred.explained_variance_ratio_)*100)\n",
    "    \n",
    "    if MAX_PCS > 0:\n",
    "        plt.xticks(PC_values[:MAX_PCS])\n",
    "        plt.xlim(0, max(PC_values[:MAX_PCS])+1)\n",
    "    plt.title('Scree Plot')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Variance Explained (%)')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'./vis/{OUTNAME}_items_PCA{suffix}.png', dpi=DPI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove"
    ]
   },
   "source": [
    "df_item.query(\"scale=='dsif'\").groupby('scalexdifficulty').matrix.count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from cluster_utils import compute_clustering_indices\n",
    "\n",
    "grouping_variable = 'scalexdifficulty'\n",
    "target_variable = 'matrix'\n",
    "minsamples = 10\n",
    "scores_dict = {'CH': [], 'DB':[], 'N_LEVELS':[], 'N_SAMPLES':[]}\n",
    "\n",
    "for perm in range(NPERMS):\n",
    "    print(perm)\n",
    "    scores = compute_clustering_indices(model, train_data, df_item, device, grouping_variable, \n",
    "                                        target_variable, shuffle=perm>0, seed=0, minsamples=minsamples)\n",
    "    [ scores_dict[key].append(scores[key]) for key in scores_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scores_df = pd.DataFrame(scores_dict['CH'])\n",
    "scores_df['perm'] = scores_df.index     \n",
    "scores_df = pd.melt(scores_df, id_vars='perm', value_name='index', var_name=grouping_variable)\n",
    "\n",
    "scores_df['N_LEVELS'] = scores_df[grouping_variable].apply(lambda x: scores_dict['N_LEVELS'][0][x])\n",
    "scores_df['N_SAMPLES'] = scores_df[grouping_variable].apply(lambda x: scores_dict['N_SAMPLES'][0][x])\n",
    "\n",
    "fig = plt.subplots(ncols=2, nrows=1)\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "scores_df['N_LEVELS'].hist(ax=ax, bins=30)\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "scores_df['N_SAMPLES'].hist(ax=ax, bins=30)        \n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "88516cc94b965045253aac22be7e673e07faa374a8dfeab45aefc65ddf94d8b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
