{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathpy as pp\n",
    "\n",
    "def generate_bipartite_block_probs(dict_p_stud_to_task):\n",
    "    n_groups_from_dict_values = set(len(v) for v in dict_p_stud_to_task.values())\n",
    "    assert len(n_groups_from_dict_values)==1, \"Input dict_p_stud_to_task implies varying number of groups\"\n",
    "    n_groups= list(n_groups_from_dict_values)[0]\n",
    "    assert n_groups==len(dict_p_stud_to_task), \"not enough keys\"\n",
    "\n",
    "    bip_block_probs = np.zeros((2*n_groups, 2*n_groups))\n",
    "    # Fill in the probabilities within and between groups\n",
    "    # Only filling probs of students to interact with tasks. \n",
    "    # Since the interactions are symmetric, the information on tasks interacting with studetns is redundant and can be obtained by summing with the transpose (see below )\n",
    "    #\n",
    "    # even indexes for students\n",
    "    for row in range(0,2*n_groups,2):\n",
    "        # odd indexes are for tasks\n",
    "        for col in range(1,2*n_groups,2):\n",
    "            stud_ix = row//2\n",
    "            task_ix = int(col/2)\n",
    "            bip_block_probs[row,col] = dict_p_stud_to_task[stud_ix][task_ix]\n",
    "    return bip_block_probs + bip_block_probs.T\n",
    "\n",
    "\n",
    "\n",
    "def sample_dcsbm(num_nodes, B, expected_degrees):\n",
    "    \"\"\"\n",
    "    Samples a network from a degree-corrected stochastic block model\n",
    "    \"\"\"\n",
    "    # assert False, \"check that num blcks an block matrix agree\"\n",
    "    assert B.shape[0] == B.shape[1], \"B should be square\"\n",
    "    num_blocks = B.shape[0]\n",
    "    m = np.random.choice(num_blocks, size=num_nodes)\n",
    "\n",
    "    # generate edges\n",
    "    edge_list = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i+1, num_nodes):\n",
    "            p_edge = B[m[i], m[j]]*expected_degrees[i]*expected_degrees[j]/(sum(expected_degrees))\n",
    "            p_edge = min(p_edge, 1)\n",
    "            assert p_edge <= 1, f\"Edge probability larger than 1: {p_edge}\"\n",
    "            if np.random.rand() < p_edge:\n",
    "                edge_list.append((i, j))\n",
    "\n",
    "    # Create a graph and add nodes\n",
    "    # network = pp.Network()\n",
    "    # for u,v in edge_list:\n",
    "    #     network.add_edge(u,v)\n",
    "    \n",
    "    # return network\n",
    "    return edge_list, m\n",
    "\n",
    "# num_nodes = 500\n",
    "# # dictionary with stud index-topic as key, and list of probabiliy of interacting with task of other index-topics\n",
    "# dict_p_stud_to_task = {}\n",
    "# dict_p_stud_to_task[0] = [1,0,0]#,.00005]\n",
    "# dict_p_stud_to_task[1] = [0,1,0]#,.001]\n",
    "# dict_p_stud_to_task[2] = [0,0,1]\n",
    "# bip_block_probs = generate_bipartite_block_probs(dict_p_stud_to_task)\n",
    "# # print(bip_block_probs)\n",
    "\n",
    "\n",
    "# theta = np.random.randint(1,num_nodes//3,num_nodes) #np.random.uniform(low=1, high=1, size=num_nodes)  # Random \n",
    "# # theta = [int(np.random.exponential(20)) for _ in range(num_nodes)]# np.random.randint(1,100,num_nodes) #np.random.uniform(low=1, high=1, size=num_nodes)  # Random \n",
    "# edges,labels = sample_dcsbm(num_nodes, bip_block_probs, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from manage_experiments import *\n",
    "\n",
    "from utils import generate_multidimensional_data_object_synthetic_geometric\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_indices = [j for i,j in zip(labels, range(len(labels))) if i%2==0]\n",
    "# student_reindexed = {j:i for i,j in enumerate(student_indices)}\n",
    "# task_indices = [j for i,j in zip(labels, range(len(labels))) if i%2==1]\n",
    "# task_reindexed = {j:i for i,j in enumerate(task_indices)}\n",
    "# n_students = len(student_indices)\n",
    "# n_tasks = len(task_indices)\n",
    "\n",
    "# edge_reordered_reindexed = []\n",
    "# for edge in edges:\n",
    "#     if edge[0] in student_indices:\n",
    "#         e = (student_reindexed[edge[0]], task_reindexed[edge[1]])\n",
    "#     else:\n",
    "#         e = (student_reindexed[edge[1]], task_reindexed[edge[0]])\n",
    "#     edge_reordered_reindexed.append(e)\n",
    "\n",
    "# # mapping groups to new indexing and node type separation\n",
    "# student_groups = np.zeros(n_students,dtype=int)\n",
    "# tasks_groups = np.zeros(n_tasks, dtype=int)\n",
    "\n",
    "# for old_ix, new_ix in student_reindexed.items():\n",
    "#     student_groups[new_ix] = labels[old_ix]\n",
    "\n",
    "# for old_ix, new_ix in task_reindexed.items():\n",
    "#     tasks_groups[new_ix] = labels[old_ix]\n",
    "\n",
    "# # Maiking students interact with tasks according to generated topology\n",
    "# p_pass_in = .9\n",
    "# p_pass_out = .1\n",
    "# y = []\n",
    "# for s_ix, t_ix in edge_reordered_reindexed:\n",
    "#     if student_groups[s_ix] == tasks_groups[t_ix]:\n",
    "#         p = p_pass_in\n",
    "#     else:\n",
    "#         p = p_pass_out\n",
    "#     y.append(np.random.binomial(1,p))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "def create_data_object_SBM(edges, labels, bip_block_p_pass_probs):\n",
    "\n",
    "    student_indices = [j for i,j in zip(labels, range(len(labels))) if i%2==0]\n",
    "    student_reindexed = {j:i for i,j in enumerate(student_indices)}\n",
    "    task_indices = [j for i,j in zip(labels, range(len(labels))) if i%2==1]\n",
    "    task_reindexed = {j:i for i,j in enumerate(task_indices)}\n",
    "    n_students = len(student_indices)\n",
    "    n_tasks = len(task_indices)\n",
    "\n",
    "    edge_reordered_reindexed = []\n",
    "    for edge in edges:\n",
    "        if edge[0] in student_indices:\n",
    "            e = (student_reindexed[edge[0]], task_reindexed[edge[1]])\n",
    "        else:\n",
    "            e = (student_reindexed[edge[1]], task_reindexed[edge[0]])\n",
    "        edge_reordered_reindexed.append(e)\n",
    "\n",
    "    # mapping groups to new indexing and node type separation\n",
    "    student_groups = np.zeros(n_students,dtype=int)\n",
    "    tasks_groups = np.zeros(n_tasks, dtype=int)\n",
    "\n",
    "    for old_ix, new_ix in student_reindexed.items():\n",
    "        student_groups[new_ix] = labels[old_ix]\n",
    "\n",
    "    for old_ix, new_ix in task_reindexed.items():\n",
    "        tasks_groups[new_ix] = labels[old_ix]\n",
    "\n",
    "    # Maiking students interact with tasks according to generated topology\n",
    "    # p_pass_in = .9\n",
    "    # p_pass_out = .1\n",
    "    y = []\n",
    "    for s_ix, t_ix in edge_reordered_reindexed:\n",
    "        # if student_groups[s_ix] == tasks_groups[t_ix]:\n",
    "        #     p = p_pass_in\n",
    "        # else:\n",
    "        #     p = p_pass_out\n",
    "        y.append(np.random.binomial(1,bip_block_p_pass_probs[student_groups[s_ix], tasks_groups[t_ix]]))\n",
    "    \n",
    "\n",
    "    data  = HeteroData()\n",
    "\n",
    "    # Save node indices\n",
    "    data['student'].node_id = torch.arange(n_students)\n",
    "    data['item'].node_id = torch.arange(n_tasks)\n",
    "\n",
    "\n",
    "    # Add the node features\n",
    "    data['student'].x= torch.eye(n_students)\n",
    "    data['item'].x = torch.eye(n_tasks)\n",
    "\n",
    "    # Add the edge indices\n",
    "    data['student', 'responds', 'item'].edge_index = torch.from_numpy(np.array(edge_reordered_reindexed).T).to(torch.long)\n",
    "\n",
    "    #add the edge attrs\n",
    "    data['student', 'responds', 'item'].edge_attr = torch.tensor([1]*len(y)).to(torch.float).reshape(-1,1)\n",
    "\n",
    "    # Add the edge label\n",
    "    data['student', 'responds', 'item'].y = torch.from_numpy(np.array(y)).to(torch.long)\n",
    "\n",
    "    # We use T.ToUndirected() to add the reverse edges from subject to students \n",
    "    # in order to let GNN pass messages in both ways\n",
    "    data = T.ToUndirected()(data)\n",
    "    del data['item', 'rev_responds', 'student'].edge_attr  # Remove 'reverse' label.\n",
    "    del data['item', 'rev_responds', 'student'].y  # Remove 'reverse' label.\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_data_object_SBM(num_nodes, bip_block_probs, theta, bip_block_p_pass_probs):\n",
    "    edges, labels = sample_dcsbm(num_nodes, bip_block_probs, theta) #generate_observations_tree_of_topics(n, n_students, n_tasks, tree_depth,branching_factor,p_ER,skill_mismatch_penalty)\n",
    "    data = create_data_object_SBM(edges, labels, bip_block_p_pass_probs)\n",
    "    # print(\"network density\",data.num_edges/(2*n_students*n_tasks))\n",
    "    return data\n",
    "\n",
    "\n",
    "# data_object = generate_data_object_SBM(num_nodes, bip_block_probs, theta)\n",
    "# data_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def line_plot_with_std(list_scores, parameters, fname):\n",
    "\n",
    "    # Convert the list of lists to a Pandas DataFrame\n",
    "    df = pd.DataFrame(list_scores)\n",
    "\n",
    "    # Calculate the mean and standard deviation of each row\n",
    "    mean = df.mean(axis=1)\n",
    "    std = df.std(axis=1)\n",
    "\n",
    "    # Create a figure and an axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the mean line\n",
    "    ax.plot(mean, color='blue', label='Mean')\n",
    "\n",
    "    # Plot the shaded region around the mean line\n",
    "    ax.fill_between(df.index, mean-std, mean+std, color='lightblue', alpha=0.5, label='Standard deviation')\n",
    "\n",
    "    # Add some labels and a legend\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Value')\n",
    "    # ax.legend()\n",
    "\n",
    "    # change x-ticks to start from \n",
    "    n_tasks_per_student_list = list(range(\n",
    "        parameters['min_n_tasks_per_student'],\n",
    "        parameters['max_n_tasks_per_student'],\n",
    "        parameters['step_n_tasks_per_student']))\n",
    "    ax.set_xticks(range(len(n_tasks_per_student_list)), n_tasks_per_student_list, rotation=45)\n",
    "\n",
    "    # Show and save the plot\n",
    "    plt.grid()\n",
    "    plt.xlabel('Number of tasks per student')\n",
    "    plt.ylabel('Balanced Accuracy')\n",
    "    fnameplot = fname.replace('results', 'pdf')\n",
    "    plt.savefig(fnameplot)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_confusion_matrix(json_content, fname):\n",
    "    fig, ax = plt.subplots(ncols = 1, figsize=(8,6))\n",
    "    \n",
    "    plt.imshow(json_content[\"Confusion_2_test\"], interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "\n",
    "    classes = [0, 1]\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    part1, part2 = fname.split('.')\n",
    "    part1 = part1 + '_consfusion_matrix'\n",
    "    fname = '.'.join([part1, part2])\n",
    "    fnameplot = fname.replace('results', 'pdf')\n",
    "    plt.savefig(fnameplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.utils import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "def create_filename_results(parameters,fold):\n",
    "    file_name = ' '.join([f\"{str(key)}-{str(value)}\" for key, value in parameters.items() if key not in ['bip_block_probs','theta','bip_block_p_pass_probs']])\n",
    "    file_name += ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(4))\n",
    "    file_name.replace(\".\",\"\")\n",
    "    file_name+=\".results\"\n",
    "    return os.path.join(fold, file_name)\n",
    "\n",
    "\n",
    "#n_students_per_task_list = [2**i for i in n_students_per_task_list]\n",
    "\n",
    "import json\n",
    "def synthetic_density_effect_runs(parameters,parameters_density, folder = 'synth_density_effect'):\n",
    "    parameters = {**parameters, **parameters_density}\n",
    "    scores = []\n",
    "    list_scores = []\n",
    "    for _ in range(parameters[\"n_runs\"]):\n",
    "        with io.capture_output() as captured:\n",
    "            data = generate_data_object_SBM(\n",
    "                num_nodes= parameters[\"num_nodes\"],\n",
    "                bip_block_probs = parameters[\"bip_block_probs\"],\n",
    "                theta = parameters[\"theta\"],\n",
    "                bip_block_p_pass_probs = parameters[\"bip_block_p_pass_probs\"]\n",
    "            )\n",
    "\n",
    "            cv_out = perform_cross_validation(data, parameters, save_embeddings=True)\n",
    "\n",
    "            score = 0\n",
    "            inner_list_scores = []\n",
    "            for fold_n in range(parameters[\"n_splits\"]):\n",
    "                score += cv_out[f\"Balanced Accuracy_{fold_n}_test\"]\n",
    "                inner_list_scores.extend([cv_out[f\"Balanced Accuracy_{fold_n}_test\"]])\n",
    "            score = score / parameters[\"n_splits\"]\n",
    "            scores.append(score)\n",
    "            list_scores.append(inner_list_scores)\n",
    "\n",
    "    # density = data.num_edges/(2*parameters[\"n_students\"]*parameters[\"n_tasks\"]) \n",
    "    res_dict = dict(zip(list(range(parameters[\"n_runs\"])),scores))\n",
    "    output_dict = {\n",
    "        **parameters,\n",
    "        # \"density\":density,\n",
    "        \"res_dict\":res_dict,\n",
    "        \"list_scores\":dict(zip(list(range(parameters[\"n_runs\"])),list_scores))\n",
    "    }\n",
    "    output_dict['bip_block_probs'] = output_dict['bip_block_probs'].tolist()\n",
    "    output_dict['theta'] = output_dict['theta'].tolist()\n",
    "    output_dict['bip_block_p_pass_probs'] = output_dict['bip_block_p_pass_probs'].tolist()\n",
    "    fname = create_filename_results(parameters_density, folder)\n",
    "    # print the types of items\n",
    "    # print({i: type(output_dict[i]) for i,j in output_dict.items()})\n",
    "    with open(fname,'w') as f:\n",
    "        json.dump(output_dict, f, skipkeys=True)\n",
    "    \n",
    "    # save results\n",
    "    # print(density)\n",
    "    print([np.mean(s) for s in list_scores])\n",
    "    # line_plot_with_std(list_scores=list_scores, parameters= parameters, fname=fname)\n",
    "\n",
    "    # # save embeddings\n",
    "    # visualise_embeddings(output_dict, fname=fname)\n",
    "\n",
    "    # visualise_embeddings_concatenated(output_dict, data['student', 'responds', 'item'].edge_index, data['student', 'item'].y, fname = fname) # plot with indices of train-test split\n",
    "    \n",
    "    # # plot with indices of train-test split\n",
    "    \n",
    "\n",
    "\n",
    "    # visualise_confusion_matrix(cv_out, fname=fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"hidden_dims\": None,\n",
    "    'model_type': None,\n",
    "    \"df_name\": \"synthetic.salamoia\",\n",
    "    \"method\": \"EdgeClassifier\",\n",
    "    \"epochs\": 1,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"weight_decay\": 0,\n",
    "    \"dropout\": 0.0,\n",
    "    \"early_stopping\": 250,\n",
    "    \"n_splits\": 10,\n",
    "    \"device\": \"cuda\",\n",
    "    \"done\": False,\n",
    "    \"batch_size\":4096,\n",
    "    #\n",
    "    }\n",
    "\n",
    "# n_groups = 3\n",
    "# # probability of connections between students and tasks\n",
    "# dict_p_stud_to_task = {}\n",
    "# dict_p_stud_to_task[0] = [1,1/(n_groups - 1),1/(n_groups - 1)]\n",
    "# dict_p_stud_to_task[1] = [1/(n_groups - 1),1,1/(n_groups - 1)]\n",
    "# dict_p_stud_to_task[2] = [1/(n_groups - 1),1/(n_groups - 1),1]\n",
    "# bip_block_probs = generate_bipartite_block_probs(dict_p_stud_to_task)\n",
    "# # probability of passing a task given the student and the task\n",
    "# dict_p_pass_stud_to_task = {}\n",
    "# dict_p_pass_stud_to_task[0] = [.8,.1,.05]\n",
    "# dict_p_pass_stud_to_task[1] = [.2,.7,.1]\n",
    "# dict_p_pass_stud_to_task[2] = [.1,.3,.6]\n",
    "# bip_block_p_pass_probs = generate_bipartite_block_probs(dict_p_stud_to_task)\n",
    "\n",
    "# theta = np.random.randint(1,num_nodes//3,num_nodes)\n",
    "\n",
    "parameters_density = {\n",
    "    'n_runs': 1,\n",
    "    'num_nodes':200,\n",
    "    'bip_block_probs':None,\n",
    "    'bip_block_p_pass_probs':None,\n",
    "    'theta':None,\n",
    "    'n_groups':None,\n",
    "    'theta_type':None,\n",
    "    'p_pass_out': None,\n",
    "    'p_pass_in': None,\t\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: IRT hidden_dims: 3 n_groups: 3 theta: lognormal mul_p_connection: 1 p_pass_out: 0.1\n",
      "[0.4988080583168246]\n",
      "Model type: GNN hidden_dims: [16, 16, 8] n_groups: 3 theta: lognormal mul_p_connection: 1 p_pass_out: 0.1\n",
      "[0.4963452380952381]\n",
      "Model type: IRT hidden_dims: 3 n_groups: 3 theta: lognormal mul_p_connection: 1 p_pass_out: 0.3\n",
      "[0.5066463810492715]\n",
      "Model type: GNN hidden_dims: [16, 16, 8] n_groups: 3 theta: lognormal mul_p_connection: 1 p_pass_out: 0.3\n",
      "[0.5175978908231532]\n",
      "Model type: IRT hidden_dims: 3 n_groups: 3 theta: lognormal mul_p_connection: 1 p_pass_out: 0.5\n",
      "[0.5267734220111862]\n",
      "Model type: GNN hidden_dims: [16, 16, 8] n_groups: 3 theta: lognormal mul_p_connection: 1 p_pass_out: 0.5\n",
      "[0.4950449335147611]\n",
      "Model type: IRT hidden_dims: 3 n_groups: 3 theta: lognormal mul_p_connection: 1 p_pass_out: 0.7\n",
      "[0.5321090452475083]\n",
      "Model type: GNN hidden_dims: [16, 16, 8] n_groups: 3 theta: lognormal mul_p_connection: 1 p_pass_out: 0.7\n",
      "[0.4936394442834605]\n",
      "Model type: IRT hidden_dims: 3 n_groups: 3 theta: lognormal mul_p_connection: 0.5 p_pass_out: 0.1\n",
      "[0.5493160476920067]\n",
      "Model type: GNN hidden_dims: [16, 16, 8] n_groups: 3 theta: lognormal mul_p_connection: 0.5 p_pass_out: 0.1\n",
      "[0.4784377820247386]\n",
      "Model type: IRT hidden_dims: 3 n_groups: 3 theta: lognormal mul_p_connection: 0.5 p_pass_out: 0.3\n",
      "[0.5106205678379592]\n",
      "Model type: GNN hidden_dims: [16, 16, 8] n_groups: 3 theta: lognormal mul_p_connection: 0.5 p_pass_out: 0.3\n",
      "[0.4541791541791541]\n",
      "Model type: IRT hidden_dims: 3 n_groups: 3 theta: lognormal mul_p_connection: 0.5 p_pass_out: 0.5\n",
      "[0.5370446775446776]\n",
      "Model type: GNN hidden_dims: [16, 16, 8] n_groups: 3 theta: lognormal mul_p_connection: 0.5 p_pass_out: 0.5\n",
      "[0.46709097786271697]\n",
      "Model type: IRT hidden_dims: 3 n_groups: 3 theta: lognormal mul_p_connection: 0.5 p_pass_out: 0.7\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_dims\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hidden_dims\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel type:\u001b[39m\u001b[38;5;124m'\u001b[39m, model_type, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dims:\u001b[39m\u001b[38;5;124m'\u001b[39m, hidden_dims, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_groups:\u001b[39m\u001b[38;5;124m'\u001b[39m, n_groups, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheta:\u001b[39m\u001b[38;5;124m'\u001b[39m, theta[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmul_p_connection:\u001b[39m\u001b[38;5;124m'\u001b[39m, mul_p_connection, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp_pass_out:\u001b[39m\u001b[38;5;124m'\u001b[39m, p_pass_out)\n\u001b[0;32m---> 35\u001b[0m \u001b[43msynthetic_density_effect_runs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mparameters_density\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters_density\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m, in \u001b[0;36msynthetic_density_effect_runs\u001b[0;34m(parameters, parameters_density, folder)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mcapture_output() \u001b[38;5;28;01mas\u001b[39;00m captured:\n\u001b[1;32m     22\u001b[0m     data \u001b[38;5;241m=\u001b[39m generate_data_object_SBM(\n\u001b[1;32m     23\u001b[0m         num_nodes\u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     24\u001b[0m         bip_block_probs \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbip_block_probs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     25\u001b[0m         theta \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheta\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     26\u001b[0m         bip_block_p_pass_probs \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbip_block_p_pass_probs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m     )\n\u001b[0;32m---> 29\u001b[0m     cv_out \u001b[38;5;241m=\u001b[39m \u001b[43mperform_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     32\u001b[0m     inner_list_scores \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Modelling-Students-Learning/manage_experiments.py:198\u001b[0m, in \u001b[0;36mperform_cross_validation\u001b[0;34m(data, parameters, save_embeddings, save_subgraph, model, final_fit)\u001b[0m\n\u001b[1;32m    195\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    197\u001b[0m val_b \u001b[38;5;241m=\u001b[39m test_loop(model, val_subgraph_data, fold, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 198\u001b[0m test_b \u001b[38;5;241m=\u001b[39m \u001b[43mtest_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_subgraph_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_b[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBalanced Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_val\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m best_val_acc:\n\u001b[1;32m    201\u001b[0m     early_stopping \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/GNN/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Modelling-Students-Learning/IRT.py:113\u001b[0m, in \u001b[0;36mtest_IRT\u001b[0;34m(model, data, fold, type)\u001b[0m\n\u001b[1;32m    108\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m    109\u001b[0m             data\u001b[38;5;241m=\u001b[39mdata\n\u001b[1;32m    110\u001b[0m             )\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    111\u001b[0m target \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 113\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {k\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m:v \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    116\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m fold\n",
      "File \u001b[0;32m~/Modelling-Students-Learning/utils.py:105\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[0;34m(y_true, pred)\u001b[0m\n\u001b[1;32m    102\u001b[0m y_predsoft \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;66;03m#softmax(pred).numpy()[:, 1]\u001b[39;00m\n\u001b[1;32m    103\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mround()\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;66;03m#.argmax(dim=1, keepdim=True).view(-1).numpy()\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m--> 105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUC\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_predsoft\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfusion\u001b[39m\u001b[38;5;124m'\u001b[39m:confusion_matrix(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;66;03m# 'F1-score-weighted':f1_score(y_true, y_pred, average='weighted'), #\u001b[39;00m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;66;03m# 'F1-score-macro':f1_score(y_true, y_pred, average='macro'),\u001b[39;00m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;66;03m# 'F1-score-micro':f1_score(y_true, y_pred, average='micro'),\u001b[39;00m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;66;03m# 'Accuracy':accuracy_score(y_true, y_pred),\u001b[39;00m\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBalanced Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: balanced_accuracy_score(y_true, y_pred),\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;66;03m# 'Precision-weighted':precision_score(y_true, y_pred, average='weighted'), #\u001b[39;00m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;66;03m# 'Precision-macro':precision_score(y_true, y_pred, average='macro'),\u001b[39;00m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;66;03m# 'Precision-micro':precision_score(y_true, y_pred, average='micro'),\u001b[39;00m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;66;03m# 'Recall-weighted':recall_score(y_true, y_pred, average='weighted'), #\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;66;03m# 'Recall-macro':recall_score(y_true, y_pred, average='macro'),\u001b[39;00m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;66;03m# 'Recall-micro':recall_score(y_true, y_pred, average='micro'),\u001b[39;00m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;66;03m# 'AMI': adjusted_mutual_info_score(y_true, y_pred)\u001b[39;00m\n\u001b[1;32m    119\u001b[0m         }\n",
      "File \u001b[0;32m~/anaconda3/GNN/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/GNN/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:627\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    625\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n\u001b[1;32m    626\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m label_binarize(y_true, classes\u001b[38;5;241m=\u001b[39mlabels)[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[1;32m    636\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[1;32m    637\u001b[0m         y_true,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    640\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    641\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/GNN/lib/python3.8/site-packages/sklearn/metrics/_base.py:75\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m     78\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true)\n",
      "File \u001b[0;32m~/anaconda3/GNN/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:382\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Binary roc auc score.\"\"\"\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_true)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not defined in that case.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m     )\n\u001b[1;32m    387\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_fpr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "# NB: having only the leaves might make it easier for IRT since it just creates separated bins (i.e., the leaves) and easy tasks (the ones sampled close to the root node)\n",
    "#  Instead, with a GNN we want to leverage the ...ability to encode the underlying tree structure (HOW might the GNN do it, specifically?)\n",
    "std = parameters_density[\"num_nodes\"]/100\n",
    "for n_groups in [3,4,5]:\n",
    "    parameters_density['n_groups'] = n_groups\n",
    "    for theta in [(np.random.lognormal(mean=np.log(20),sigma=np.log(2),size=parameters_density[\"num_nodes\"]), 'lognormal'), (np.random.normal(parameters_density[\"num_nodes\"]/n_groups, std, parameters_density[\"num_nodes\"]), \"gaussian\"), (np.random.randint(1,parameters_density[\"num_nodes\"]//n_groups,parameters_density[\"num_nodes\"]), \"uniform\")]:#, (np.random.exponential(100,parameters_density[\"num_nodes\"]), \"exponential\")]:\n",
    "        parameters_density['theta'] = theta[0]\n",
    "        parameters_density['theta_type'] = theta[1]\n",
    "        for mul_p_connection in [1, 1/2, 1/3, 2/3]:\n",
    "            parameters_density['mul_p_connection'] = mul_p_connection\n",
    "            dict_p_stud_to_task = {}\n",
    "            for i in range(n_groups):\n",
    "                dict_p_stud_to_task[i] = [mul_p_connection/(n_groups-1)]*n_groups\n",
    "                dict_p_stud_to_task[i][i] = 1\n",
    "            bip_block_probs = generate_bipartite_block_probs(dict_p_stud_to_task)\n",
    "            parameters_density['bip_block_probs'] = bip_block_probs\n",
    "            parameters_density['p_pass_in'] = 1\n",
    "            for p_pass_out in [.1, .3, .5, .7]:\n",
    "                dict_p_pass_stud_to_task = {}\n",
    "                for i in range(n_groups):\n",
    "                    dict_p_pass_stud_to_task[i] = [p_pass_out]*n_groups\n",
    "                    dict_p_pass_stud_to_task[i][i] = 1\n",
    "                parameters_density['p_pass_out'] = p_pass_out\n",
    "                bip_block_p_pass_probs = generate_bipartite_block_probs(dict_p_pass_stud_to_task)\n",
    "                parameters_density['bip_block_p_pass_probs'] = bip_block_p_pass_probs\n",
    "                for model_type,hidden_dims in [\n",
    "                    (\"IRT\",3),\n",
    "                    (\"GNN\", [\n",
    "                        16,\n",
    "                        16,\n",
    "                        8])]:\n",
    "                    parameters[\"model_type\"] = model_type\n",
    "                    parameters[\"hidden_dims\"] = hidden_dims\n",
    "                    print('Model type:', model_type, 'hidden_dims:', hidden_dims, 'n_groups:', n_groups, 'theta:', theta[1], 'mul_p_connection:', mul_p_connection, 'p_pass_out:', p_pass_out)\n",
    "                    synthetic_density_effect_runs(parameters=parameters,\n",
    "                                                parameters_density=parameters_density,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def read_json_files(directory):\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".results\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "                data.append(json_data)\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_json_files('synth_density_effect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['list_scores_latex'] = df['list_scores'].apply(lambda x: f\"{np.mean([np.mean(v)*100 for v in x.values()]):.2f} Â± {np.std([np.mean(v)*100 for v in x.values()]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\n",
    "    'p_pass_out', \n",
    "    # 'bip_block_p_pass_probs', \n",
    "    'theta_type', \n",
    "    'n_groups', \n",
    "    'mul_p_connection',\n",
    "    'model_type'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(group_cols).agg({\n",
    "    'list_scores': lambda x: x.apply(lambda y: [np.mean(v)*100 for v in y.values()]),\n",
    "    }).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_selector(df, cols, values):\n",
    "    mask = np.full(len(df), True)\n",
    "    for col, value in zip(cols, values):\n",
    "        mask = mask & (df[col] == value)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_line_graph(df, xaxis_col, list_scores_col, label=\"\"):\n",
    "    # Set the style of the plot\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    # Set color palette\n",
    "    sns.set_palette(\"colorblind\")\n",
    "\n",
    "    # Create the plot\n",
    "    x_values = df[xaxis_col]\n",
    "    y_mean = df[list_scores_col].apply(lambda x: np.mean(x))\n",
    "    y_std = df[list_scores_col].apply(lambda x: np.std(x))\n",
    "\n",
    "    # Create an error bar plot with a line marker style\n",
    "    plt.errorbar(x_values, y_mean, yerr=y_std, fmt='-o', label=label, capsize=5)\n",
    "\n",
    "    # Set the labels and title\n",
    "    plt.xlabel(xaxis_col, fontsize=16)\n",
    "    plt.ylabel('Mean Scores', fontsize=16)\n",
    "    plt.title('Line Graph', fontsize=20)\n",
    "    plt.ylim(20, 100)\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend(fontsize=14)\n",
    "\n",
    "    # Show the plot\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select_GNN = grouped[col_selector(grouped, ['theta_type', 'n_groups', 'mul_p_connection', 'model_type'], ['lognormal', 3, 1/2, 'GNN'])]\n",
    "df_select_IRT = grouped[col_selector(grouped, ['theta_type', 'n_groups', 'mul_p_connection', 'model_type'], ['lognormal', 3, 1/2, 'IRT'])]\n",
    "\n",
    "plot_line_graph(df_select_GNN, 'p_pass_out', 'list_scores', label='GNN')\n",
    "plot_line_graph(df_select_IRT, 'p_pass_out', 'list_scores', label='IRT')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped['Mean'] = grouped['list_scores'].apply(lambda x: round(np.mean(x),2))\n",
    "grouped['Std'] = grouped['list_scores'].apply(lambda x: round(np.std(x),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for group, df_group in grouped.groupby(['n_groups', 'theta_type', 'mul_p_connection', 'p_pass_out']):\n",
    "    try:\n",
    "        gain = (df_group[df_group['model_type'] == 'GNN']['Mean'].values[0] - df_group[df_group['model_type'] == 'IRT']['Mean'].values[0]) / df_group[df_group['model_type'] == 'IRT']['Mean'].values[0]*100\n",
    "        std = np.sqrt((df_group[df_group['model_type'] == 'GNN']['Std'].values[0])**2 + (df_group[df_group['model_type'] == 'IRT']['Std'].values[0])**2)\n",
    "        df_list.append(pd.DataFrame({\n",
    "            'n_groups': [group[0]],\n",
    "            'theta_type': [group[1]],\n",
    "            'mul_p_connection': [group[2]],\n",
    "            'p_pass_out': [group[3]],\n",
    "            'gain': [gain],\n",
    "            'std': [std]\t\n",
    "        }))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mul_p_connection = 1\n",
    "\n",
    "\n",
    "theta_types = ['gaussian', 'uniform', 'lognormal']\n",
    "n_groups = [3, 4, 5]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    len(n_groups),\n",
    "    len(theta_types),\n",
    "    figsize=(15, 15),\n",
    "    sharey=True\n",
    "    )\n",
    "for theta_type in theta_types: \n",
    "    for ng in n_groups:\n",
    "        col = theta_types.index(theta_type)\n",
    "        row = n_groups.index(ng)\n",
    "        sub_new_df = new_df[\n",
    "            (new_df['theta_type'] == theta_type) & \n",
    "            (new_df['n_groups'] == ng) & \n",
    "            (new_df['mul_p_connection'] == mul_p_connection)\n",
    "            ]\n",
    "        \n",
    "        \n",
    "        # sns.barplot(data=sub_new_df, x='p_pass_out', y='gain', ax = axes[row,col])\n",
    "        # axes[row,col].set_title(f\" {theta_type}, {ng} groups\")\n",
    "        # color the bars: redu if positive gain, blue if negative\n",
    "        sub_new_df = sub_new_df.copy()\n",
    "        sub_new_df['color'] = sub_new_df['gain'].apply(lambda x: 'blue' if x>0 else 'red')\n",
    "        axes[row, col].bar(x=sub_new_df['p_pass_out'], height=sub_new_df['gain'], yerr=sub_new_df['std'], color=sub_new_df['color'].values.tolist(), width=0.1, align='center')\n",
    "        axes[row,col].set_title(f\" {theta_type}, {ng} groups\")\n",
    "        axes[row,col].set_xlabel('p_pass_out')\n",
    "        axes[row,col].set_ylabel('Gain (%)')\n",
    "        axes[row,col].set_ylim(-20, 20)\n",
    "\n",
    "# theta_type = 'uniform'\n",
    "# n_groups = 3\n",
    "\n",
    "# sub_new_df = new_df[(new_df['theta_type'] == theta_type) & (new_df['n_groups'] == n_groups) & (new_df['mul_p_connection'] == mul_p_connection)]\n",
    "\n",
    "\n",
    "# fig, axes = plt.subplots(1, 1, figsize=(10, 15))\n",
    "\n",
    "\n",
    "# sns.barplot(data=sub_new_df, x='p_pass_out', y='gain', ax = axes)\n",
    "# axes.set_title(\"fucl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_gaussian = new_df[new_df['theta_type'] == 'gaussian']\n",
    "new_df_lognormal = new_df[new_df['theta_type'] == 'lognormal']\n",
    "new_df_uniform = new_df[new_df['theta_type'] == 'uniform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 15))\n",
    "\n",
    "sns.barplot(x='n_groups', y='gain', data=new_df_gaussian, ax=axes[0,0])\n",
    "axes[0,0].set_title('n_groups')\n",
    "\n",
    "sns.barplot(x='mul_p_connection', y='gain', data=new_df_gaussian, ax=axes[1,0])\n",
    "axes[1,0].set_title('mul_p_connection')\n",
    "\n",
    "sns.barplot(x='p_pass_out', y='gain', data=new_df_gaussian, ax=axes[2,0])\n",
    "axes[2,0].set_title('p_pass_out')\n",
    "\n",
    "\n",
    "sns.barplot(x='n_groups', y='gain', data=new_df_uniform, ax=axes[0,1])\n",
    "axes[0,1].set_title('n_groups')\n",
    "\n",
    "sns.barplot(x='mul_p_connection', y='gain', data=new_df_uniform, ax=axes[1,1])\n",
    "axes[1,1].set_title('mul_p_connection')\n",
    "\n",
    "sns.barplot(x='p_pass_out', y='gain', data=new_df_uniform, ax=axes[2,1])\n",
    "axes[2,1].set_title('p_pass_out')\n",
    "\n",
    "\n",
    "sns.barplot(x='n_groups', y='gain', data=new_df_lognormal, ax=axes[0,2])\n",
    "axes[0,2].set_title('n_groups')\n",
    "\n",
    "sns.barplot(x='mul_p_connection', y='gain', data=new_df_lognormal, ax=axes[1,2])\n",
    "axes[1,2].set_title('mul_p_connection')\n",
    "\n",
    "sns.barplot(x='p_pass_out', y='gain', data=new_df_lognormal, ax=axes[2,2])\n",
    "axes[2,2].set_title('p_pass_out')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes=500\n",
    "mu = 20\n",
    "plt.hist(np.random.lognormal(mean=np.log(mu),sigma=np.log(2),size=n_nodes), bins = 100)\n",
    "plt.vlines(x=mu,ymin=0,ymax=15,colors=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_object_SBM(num_nodes, bip_block_probs, theta, bip_block_p_pass_probs):\n",
    "    edges, labels = sample_dcsbm(num_nodes, bip_block_probs, theta) #generate_observations_tree_of_topics(n, n_students, n_tasks, tree_depth,branching_factor,p_ER,skill_mismatch_penalty)\n",
    "    data = create_data_object_SBM(edges, labels, bip_block_p_pass_probs)\n",
    "    # print(\"network density\",data.num_edges/(2*n_students*n_tasks))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 500\n",
    "theta = np.random.lognormal(mean=np.log(20),sigma=np.log(2),size=num_nodes)\n",
    "n_groups = 3\n",
    "mul_p_connection = 1/2\n",
    "dict_p_stud_to_task = {}\n",
    "for i in range(n_groups):\n",
    "    dict_p_stud_to_task[i] = [mul_p_connection/(n_groups-1)]*n_groups\n",
    "    dict_p_stud_to_task[i][i] = 1\n",
    "bip_block_probs = generate_bipartite_block_probs(dict_p_stud_to_task)\n",
    "\n",
    "p_pass_out = .1\n",
    "dict_p_pass_stud_to_task = {}\n",
    "for i in range(n_groups):\n",
    "    dict_p_pass_stud_to_task[i] = [p_pass_out]*n_groups\n",
    "    dict_p_pass_stud_to_task[i][i] = 1\n",
    "parameters_density['p_pass_out'] = p_pass_out\n",
    "bip_block_p_pass_probs = generate_bipartite_block_probs(dict_p_pass_stud_to_task)\n",
    "\n",
    "data = generate_data_object_SBM(num_nodes, bip_block_probs, theta, bip_block_p_pass_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_val in grouped.p_pass_out.unique():\n",
    "    print(p_val,grouped[grouped.p_pass_out == p_val].n_groups.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_val in grouped.p_pass_out.unique():\n",
    "    print(grouped[grouped.p_pass_out == p_val].mul_p_connection.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped[grouped.p_pass_out == p_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, alpha):\n",
    "    return 1/(1+np.exp(-x/alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-9,10)\n",
    "plt.plot(x, [sigmoid(xi,0) for xi in x], label=\"alpha = 0\")\n",
    "plt.plot(x, [sigmoid(xi,1) for xi in x], label=\"alpha = 1\")\n",
    "plt.plot(x, [sigmoid(xi,2) for xi in x], label=\"alpha = 2\")\n",
    "plt.plot(x, [sigmoid(xi,3) for xi in x], label=\"alpha = 3\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathpy as pp\n",
    "from collections import defaultdict \n",
    "import copy\n",
    "# tree\n",
    "root = \"*\"\n",
    "max_depth = 3\n",
    "nodes_at_layer = defaultdict(list)\n",
    "nodes_at_layer[0].append(root)\n",
    "start_nodes = [\"0\",\"1\"]\n",
    "nodes_at_layer[1].extend(start_nodes)\n",
    "for depth in range(2,max_depth+1):\n",
    "    # nodes_this_layer = []\n",
    "    # nodes_prev_layer = copy.deepcopy(nodes_at_layer[depth-1])\n",
    "    for node in nodes_at_layer[depth-1]:\n",
    "        for base_node in start_nodes:\n",
    "            nodes_at_layer[depth].append(node+base_node)\n",
    "\n",
    "\n",
    "nodes_at_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = pp.Network(directed = True)\n",
    "for depth in range(max(nodes_at_layer)):\n",
    "    if depth == 0:\n",
    "        for node in nodes_at_layer[1]:\n",
    "            n.add_edge(root,node)\n",
    "    else:\n",
    "        for node_parent in nodes_at_layer[depth]:\n",
    "            for node_child in nodes_at_layer[depth+1]:\n",
    "                if node_child[:-1]==node_parent:\n",
    "                    n.add_edge(node_parent,node_child)\n",
    "print(n.ncount())\n",
    "pp.visualisation.plot(n, width = 800,height = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(n.nodes)\n",
    "n_items = 100\n",
    "# NB: randomly sampling nodes will disproportionately give leaves, i.e., difficult items.\n",
    "item_positions = np.random.choice(nodes,size=n_items)\n",
    "dict_item_positions = dict(zip(range(n_items),item_positions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_skills(n, study_hours,root=\"*\"):\n",
    "    # gives skills to students\n",
    "    # we start by taking one random path from root to leaf for each student\n",
    "    # extensions goes in the direction of using personalized pagerank from the root node and would include: \n",
    "    #   study hours (similar to ability, encodes number of transitions taken by the students rw)\n",
    "    #   restart probability tau at any node\n",
    "    #   once an end node is reached, student restards from prev visited node (or root) with prob 1\n",
    "    #\n",
    "    # for now just enough to reach one leaf\n",
    "    T = n.transition_matrix().todense().T\n",
    "    root_index = n.node_to_name_map()[root]\n",
    "    ix_to_node = {v:k for k,v in n.node_to_name_map().items()}\n",
    "    last_node_ix = root_index\n",
    "    for _ in range(study_hours):\n",
    "        last_node_ix = np.random.choice(range(n.ncount()),p=T[last_node_ix,:].tolist()[0])\n",
    "    return ix_to_node[last_node_ix]\n",
    "\n",
    "\n",
    "n_students = 100\n",
    "dict_students_abilities = {stud_id:[] for stud_id in range(n_students)}\n",
    "for stud_id in dict_students_abilities:\n",
    "    # append, but potentially extend cause students can get skills in different branches\n",
    "    dict_students_abilities[stud_id].append(assign_skills(n,depth+1))\n",
    "# dict_students_abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# students interact randomly with items (erdos renyi). Student pass items if they have traversed the topic/tree-node in their learning path\n",
    "p_ER = .3\n",
    "edges = []\n",
    "labels = []\n",
    "for stud_id in dict_students_abilities:\n",
    "    for item_id in dict_item_positions:\n",
    "        if np.random.rand()<p_ER:\n",
    "            edges.append((stud_id,item_id))\n",
    "            # deciding if item is passed\n",
    "            # could be made more nuanced checking how much of the path to the item_topic has been traversed by the student\n",
    "            item_topic = dict_item_positions[item_id] \n",
    "            student_skills = dict_students_abilities[stud_id]\n",
    "            if any([item_topic==skill[:len(item_topic)] for skill in student_skills if len(skill)>=len(item_topic)]):\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "print(sum(labels)/len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATING NODE POSITIONS AND PLOTTING THEM \n",
    "# NB: ALSO SETTING RADIUS USED BELOW\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "dimension = 2\n",
    "n_students = 270\n",
    "n_items = 750\n",
    "#\n",
    "student_geom = np.random.rand(n_students,dimension)\n",
    "student_ids = range(n_students)\n",
    "items_geom = np.random.rand(n_items,dimension)\n",
    "items_id = range(n_students, n_students+n_items)\n",
    "#\n",
    "radius = .1\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(student_geom[:,0],student_geom[:,1], label = \"student\")\n",
    "ax.scatter(items_geom[:,0],items_geom[:,1], label = \"item\")\n",
    "plt.xlabel(\"Student ability\")\n",
    "plt.ylabel(\"Item diffuculty\")\n",
    "circle = Circle([.5,.5], radius, fill=False, edgecolor='red')\n",
    "ax.add_patch(circle)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATING NODE POSITIONS AND PLOTTING THEM \n",
    "# NB: ALSO SETTING RADIUS USED BELOW\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "dimension = 2\n",
    "n_students = 270\n",
    "n_items = 750\n",
    "#\n",
    "student_geom = np.random.rand(n_students,dimension)\n",
    "student_ids = range(n_students)\n",
    "items_geom = np.random.rand(n_items,dimension)\n",
    "items_id = range(n_students, n_students+n_items)\n",
    "#\n",
    "radius = .1\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(student_geom[:,0],[0]*len(student_geom[:,1]), label = \"student\")\n",
    "ax.scatter([0]*len(items_geom[:,0]),items_geom[:,1], label = \"item\")\n",
    "plt.xlabel(\"Student ability\")\n",
    "plt.ylabel(\"Item diffuculty\")\n",
    "# circle = Circle([.5,.5], radius, fill=False, edgecolor='red')\n",
    "# ax.add_patch(circle)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_students = 100\n",
    "n_items = 150\n",
    "\n",
    "list_a = [np.random.randint(50)/50 for _ in range(n_students)]\n",
    "list_i = [np.random.randint(50)/50 for _ in range(n_items)]\n",
    "\n",
    "edges = []\n",
    "for i in range(n_students):\n",
    "    # ability student i\n",
    "    a_i = list_a[i]\n",
    "    for j in range(n_items):\n",
    "        # difficulty item j\n",
    "        d_j = list_i[j]\n",
    "        # delta gives how appropriate taks diff is for student ability\n",
    "        delta = abs(a_i - d_j)\n",
    "        # probability of interaction prop to inverse of delta: \n",
    "        p_i_j = 1/(2+delta)**4\n",
    "        assert p_i_j <= 1\n",
    "        if np.random.rand()<p_i_j:\n",
    "            edges.append((i,i+j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathpy as pp\n",
    "n = pp.Network(directed=True)\n",
    "for (u,v) in edges:\n",
    "    n.add_edge(u,v)\n",
    "pp.visualisation.plot(n, width =800,height = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBTAINING PROBABILITY (AND PLOTTING PATTERNS FOR ON EDGES -- POSITION TAKING AVERAGE BETWEEN NODES INVOLVED IN THE INTERACTIONS)\n",
    "label_probs = []\n",
    "list_edge_points = []\n",
    "for i in range(n_students):\n",
    "    for j in range(n_items):\n",
    "        if np.linalg.norm(student_geom[i]-items_geom[j]) < radius:\n",
    "            # first coordinate used as student ability \n",
    "            # second coordinate used as item difficulty\n",
    "            x = student_geom[i][0]-items_geom[j][1]\n",
    "            label_probs.append(1/(1+np.exp(-x)))\n",
    "            list_edge_points.append((student_geom[i]+items_geom[j])/2)\n",
    "\n",
    "list_edge_points = np.array(list_edge_points)\n",
    "plt.xlabel(\"Student ability\")\n",
    "plt.ylabel(\"Item diffuculty\")\n",
    "plt.scatter(list_edge_points[:,0],list_edge_points[:,1], c=label_probs)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # CREATING LIST OF EDGES\n",
    "# # import pathpy as pp\n",
    "# import numpy as np\n",
    "# n = pp.Network()\n",
    "# edges = []\n",
    "# for i in range(n_students):\n",
    "#     for j in range(n_items):\n",
    "#         if np.linalg.norm(student_geom[i]-items_geom[j]) < radius:\n",
    "#             edges.append((i,i+j))\n",
    "#             n.add_edge(i,n_students+j)\n",
    "\n",
    "\n",
    "# # COOL DRAGON-LIKE VISUALIZATION\n",
    "# # pp.visualisation.plot(n, width =800,height = 800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
